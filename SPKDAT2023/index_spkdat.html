<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui"><title>SPARK AVEC DATABRICKS</title><meta name="description" content="Formation Spark avec Databricks"><meta name="keywords" content="Big data, Machine Learning, Data, Azure Databricks, Apache Spark"><meta name="author" content="Dr. Amine HADJ-YOUCEF"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2/css/reset.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2/css/reveal.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2/css/theme/white.css" id="theme"><!--This CSS is generated by the Asciidoctor reveal.js converter to further integrate AsciiDoc's existing semantic with reveal.js--><style type="text/css">.reveal div.right {
  float: right
}

/* listing block */
.reveal .listingblock.stretch > .content {
  height: 100%
}

.reveal .listingblock.stretch > .content > pre {
  height: 100%
}

.reveal .listingblock.stretch > .content > pre > code {
  height: 100%;
  max-height: 100%
}

/* tables */
table {
  border-collapse: collapse;
  border-spacing: 0
}

table {
  margin-bottom: 1.25em;
  border: solid 1px #dedede
}

table thead tr th, table thead tr td, table tfoot tr th, table tfoot tr td {
  padding: .5em .625em .625em;
  font-size: inherit;
  text-align: left
}

table tr th, table tr td {
  padding: .5625em .625em;
  font-size: inherit
}

table thead tr th, table tfoot tr th, table tbody tr td, table tr td, table tfoot tr td {
  display: table-cell;
  line-height: 1.6
}

td.tableblock > .content {
  margin-bottom: 1.25em
}

td.tableblock > .content > :last-child {
  margin-bottom: -1.25em
}

table.tableblock, th.tableblock, td.tableblock {
  border: 0 solid #dedede
}

table.grid-all > thead > tr > .tableblock, table.grid-all > tbody > tr > .tableblock {
  border-width: 0 1px 1px 0
}

table.grid-all > tfoot > tr > .tableblock {
  border-width: 1px 1px 0 0
}

table.grid-cols > * > tr > .tableblock {
  border-width: 0 1px 0 0
}

table.grid-rows > thead > tr > .tableblock, table.grid-rows > tbody > tr > .tableblock {
  border-width: 0 0 1px
}

table.grid-rows > tfoot > tr > .tableblock {
  border-width: 1px 0 0
}

table.grid-all > * > tr > .tableblock:last-child, table.grid-cols > * > tr > .tableblock:last-child {
  border-right-width: 0
}

table.grid-all > tbody > tr:last-child > .tableblock, table.grid-all > thead:last-child > tr > .tableblock, table.grid-rows > tbody > tr:last-child > .tableblock, table.grid-rows > thead:last-child > tr > .tableblock {
  border-bottom-width: 0
}

table.frame-all {
  border-width: 1px
}

table.frame-sides {
  border-width: 0 1px
}

table.frame-topbot, table.frame-ends {
  border-width: 1px 0
}

.reveal table th.halign-left, .reveal table td.halign-left {
  text-align: left
}

.reveal table th.halign-right, .reveal table td.halign-right {
  text-align: right
}

.reveal table th.halign-center, .reveal table td.halign-center {
  text-align: center
}

.reveal table th.valign-top, .reveal table td.valign-top {
  vertical-align: top
}

.reveal table th.valign-bottom, .reveal table td.valign-bottom {
  vertical-align: bottom
}

.reveal table th.valign-middle, .reveal table td.valign-middle {
  vertical-align: middle
}

table thead th, table tfoot th {
  font-weight: bold
}

tbody tr th {
  display: table-cell;
  line-height: 1.6
}

tbody tr th, tbody tr th p, tfoot tr th, tfoot tr th p {
  font-weight: bold
}

thead {
  display: table-header-group
}

.reveal table.grid-none th, .reveal table.grid-none td {
  border-bottom: 0 !important
}

/* kbd macro */
kbd {
  font-family: "Droid Sans Mono", "DejaVu Sans Mono", monospace;
  display: inline-block;
  color: rgba(0, 0, 0, .8);
  font-size: .65em;
  line-height: 1.45;
  background: #f7f7f7;
  border: 1px solid #ccc;
  -webkit-border-radius: 3px;
  border-radius: 3px;
  -webkit-box-shadow: 0 1px 0 rgba(0, 0, 0, .2), 0 0 0 .1em white inset;
  box-shadow: 0 1px 0 rgba(0, 0, 0, .2), 0 0 0 .1em #fff inset;
  margin: 0 .15em;
  padding: .2em .5em;
  vertical-align: middle;
  position: relative;
  top: -.1em;
  white-space: nowrap
}

.keyseq kbd:first-child {
  margin-left: 0
}

.keyseq kbd:last-child {
  margin-right: 0
}

/* callouts */
.conum[data-value] {
  display: inline-block;
  color: #fff !important;
  background: rgba(0, 0, 0, .8);
  -webkit-border-radius: 50%;
  border-radius: 50%;
  text-align: center;
  font-size: .75em;
  width: 1.67em;
  height: 1.67em;
  line-height: 1.67em;
  font-family: "Open Sans", "DejaVu Sans", sans-serif;
  font-style: normal;
  font-weight: bold
}

.conum[data-value] * {
  color: #fff !important
}

.conum[data-value] + b {
  display: none
}

.conum[data-value]:after {
  content: attr(data-value)
}

pre .conum[data-value] {
  position: relative;
  top: -.125em
}

b.conum * {
  color: inherit !important
}

.conum:not([data-value]):empty {
  display: none
}

/* Callout list */
.hdlist > table, .colist > table {
  border: 0;
  background: none
}

.hdlist > table > tbody > tr, .colist > table > tbody > tr {
  background: none
}

td.hdlist1, td.hdlist2 {
  vertical-align: top;
  padding: 0 .625em
}

td.hdlist1 {
  font-weight: bold;
  padding-bottom: 1.25em
}

/* Disabled from Asciidoctor CSS because it caused callout list to go under the
 * source listing when .stretch is applied (see #335)
 * .literalblock+.colist,.listingblock+.colist{margin-top:-.5em} */
.colist td:not([class]):first-child {
  padding: .4em .75em 0;
  line-height: 1;
  vertical-align: top
}

.colist td:not([class]):first-child img {
  max-width: none
}

.colist td:not([class]):last-child {
  padding: .25em 0
}

/* Override Asciidoctor CSS that causes issues with reveal.js features */
.reveal .hljs table {
  border: 0
}

/* Callout list rows would have a bottom border with some reveal.js themes (see #335) */
.reveal .colist > table th, .reveal .colist > table td {
  border-bottom: 0
}

/* Fixes line height with Highlight.js source listing when linenums enabled (see #331) */
.reveal .hljs table thead tr th, .reveal .hljs table tfoot tr th, .reveal .hljs table tbody tr td, .reveal .hljs table tr td, .reveal .hljs table tfoot tr td {
  line-height: inherit
}

/* Columns layout */
.columns .slide-content {
  display: flex;
}

.columns.wrap .slide-content {
  flex-wrap: wrap;
}

.columns.is-vcentered .slide-content {
  align-items: center;
}

.columns .slide-content > .column {
  display: block;
  flex-basis: 0;
  flex-grow: 1;
  flex-shrink: 1;
}

.columns .slide-content > .column > * {
  padding: .75rem;
}

/* See #353 */
.columns.wrap .slide-content > .column {
  flex-basis: auto;
}

.columns .slide-content > .column.is-full {
  flex: none;
  width: 100%;
}

.columns .slide-content > .column.is-four-fifths {
  flex: none;
  width: 80%;
}

.columns .slide-content > .column.is-three-quarters {
  flex: none;
  width: 75%;
}

.columns .slide-content > .column.is-two-thirds {
  flex: none;
  width: 66.6666%;
}

.columns .slide-content > .column.is-three-fifths {
  flex: none;
  width: 60%;
}

.columns .slide-content > .column.is-half {
  flex: none;
  width: 50%;
}

.columns .slide-content > .column.is-two-fifths {
  flex: none;
  width: 40%;
}

.columns .slide-content > .column.is-one-third {
  flex: none;
  width: 33.3333%;
}

.columns .slide-content > .column.is-one-quarter {
  flex: none;
  width: 25%;
}

.columns .slide-content > .column.is-one-fifth {
  flex: none;
  width: 20%;
}

.columns .slide-content > .column.has-text-left {
  text-align: left;
}

.columns .slide-content > .column.has-text-justified {
  text-align: justify;
}

.columns .slide-content > .column.has-text-right {
  text-align: right;
}

.columns .slide-content > .column.has-text-left {
  text-align: left;
}

.columns .slide-content > .column.has-text-justified {
  text-align: justify;
}

.columns .slide-content > .column.has-text-right {
  text-align: right;
}

.text-left {
  text-align: left !important
}

.text-right {
  text-align: right !important
}

.text-center {
  text-align: center !important
}

.text-justify {
  text-align: justify !important
}

.footnotes {
  border-top: 1px solid rgba(0, 0, 0, 0.2);
  padding: 0.5em 0 0 0;
  font-size: 0.65em;
  margin-top: 4em;
}
</style><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/v4-shims.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
tex2jax: {
  inlineMath: [["\\(", "\\)"]],
  displayMath: [["\\[", "\\]"]],
  ignoreClass: "nostem|nolatexmath"
},
asciimath2jax: {
  delimiters: [["\\$", "\\$"]],
  ignoreClass: "nostem|noasciimath"
},
TeX: { equationNumbers: { autoNumber: "none" } }
});</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script><!--Printing and PDF exports--><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? "https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2/css/print/pdf.css" : "https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2/css/print/paper.css";
document.getElementsByTagName( 'head' )[0].appendChild( link );</script><link rel="stylesheet" href="css/custom_spkdat.css"><meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<link
  rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"
  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog=="
  crossorigin="anonymous"
/></head><body><div class="reveal"><div class="slides"><header>
  <img src="https://cdn.webikeo.com/public/media/channel/100016949/logo-m2i-rouge.jpg" >
  <img src="https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg" >
</header><section class="title" data-state="title" data-transition="fade" data-transition-speed="fast"><h1>SPARK AVEC DATABRICKS</h1><div class="preamble"><div class="paragraph"><p>01-03 Mars 2023</p></div></div><p class="author"><small>Dr. Amine HADJ-YOUCEF</small></p></section>
<section><section id="_introduction_à_la_formation"><h2>Introduction à la formation</h2><div class="slide-content"><div class="paragraph"><p><span class="image"><img src="https://www.vectorlogo.zone/logos/apache_spark/apache_spark-ar21.svg" alt="apache spark ar21" width="alt" height="200"></span>
<span class="image"><img src="https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png" alt="Databricks Logo" width="alt" height="200"></span></p></div></div></section><section id="_sommaire"><h2>Sommaire</h2><div class="slide-content"><div class="ulist"><ul><li><p>Table ronde</p></li><li><p>Émargement de la présence</p></li><li><p>Planning de formation</p></li><li><p>Objectif de la formation</p></li><li><p>Programme de la formation en détail</p></li><li><p>Matériel de formation</p></li><li><p>Support de formation</p></li></ul></div></div></section><section id="_table_ronde" class="columns is-vcentered"><h2>Table ronde</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:40%"><col style="width:60%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="media/photo_linkedin_2020.png" alt="photo linkedin 2020" height="300"></div>
<div class="ulist"><ul><li><p><a href="https://www.linkedin.com/in/aminehy/">Dr. <strong>Amine Hadj-Youcef</strong></a>
<span class="image left"><img src="https://upload.wikimedia.org/wikipedia/commons/3/3d/Logo_Universit%C3%A9_Paris-Saclay.svg" alt="Logo Universit%C3%A9 Paris Saclay" height="100"></span>
<span class="image left"><img src="https://upload.wikimedia.org/wikipedia/commons/d/de/LogoCS.png" alt="LogoCS" height="100"></span></p></li><li><p><a href="mailto:hadjyoucef.amine@gmail.com">hadjyoucef.amine@gmail.com</a></p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>​Consultant en Data Science &amp; AI</p><div class="paragraph"><p><span class="image left"><img src="https://upload.wikimedia.org/wikipedia/fr/e/ec/Logo_SNCF_R%C3%A9seau_2015.svg" alt="Logo SNCF R%C3%A9seau 2015" height="100"></span>
<span class="image center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/TAG_Heuer_logo.svg/1200px-TAG_Heuer_logo.svg.png" alt="1200px TAG Heuer logo.svg" height="100"></span>
<span class="image center"><img src="https://www.dronevolt.com/wp-content/uploads/2020/01/logo_dronevolt_menu.svg" alt="logo dronevolt menu" height="100"></span>
<span class="image center"><img src="https://www.ias.u-psud.fr/sites/default/files/logoias.jpg" alt="logoias" height="100"></span></p></div></li><li><p>Mentor professionnel et Formateur</p><div class="paragraph"><p><span class="image left"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Decathlon_Logo.svg/640px-Decathlon_Logo.svg.png" alt="640px Decathlon Logo.svg" height="100"></span>
<span class="image left"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Atos_Origin_2011_logo.svg/640px-Atos_Origin_2011_logo.svg.png" alt="640px Atos Origin 2011 logo.svg" height="100"></span>
<span class="image left"><img src="https://upload.wikimedia.org/wikipedia/commons/4/46/TF1_logo_2006.svg" alt="TF1 logo 2006" height="100"></span>
<span class="image left"><img src="https://upload.wikimedia.org/wikipedia/commons/c/ce/Logo_Bouygues_Immobilier.png" alt="Logo Bouygues Immobilier" height="100"></span>
<span class="image left"><img src="https://lucyinthecloud.com/app/uploads/2020/06/lucy-in-the-cloud-logo.png" alt="lucy in the cloud logo" height="100"></span>
<span class="image left"><img src="https://i.pinimg.com/736x/2c/a6/c8/2ca6c8d907e76428a30ce6b4aad8f6a6.jpg" alt="2ca6c8d907e76428a30ce6b4aad8f6a6" height="100"></span></p></div></li></ul></div></div></td></tr></table></div></section><section id="_table_ronde_2"><h2>Table ronde</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:40%"><col style="width:60%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Issam ALAMI</p></li><li><p>Thierry RAZAFINDRAKOTO</p></li><li><p>Franck SIEFFERT</p></li><li><p>Abdoulaye Imourana DIALLO</p></li><li><p>Mauranne COSTIER</p></li><li><p>Lina HAMOUD</p></li><li><p>Samar MAMI</p></li><li><p>Bertrand GARNIER</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="https://images.pexels.com/photos/3183183/pexels-photo-3183183.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=2" alt="pexels photo 3183183"></div></div></td></tr></table></div></section><section id="_feuille_démargement" data-transition-speed="fast"><h2>Feuille d&#8217;émargement</h2><div class="slide-content"><div class="imageblock"><img src="media/introduction/image_2023-01-30-11-32-04.png_.png" alt="image 2023 01 30 11 32 04.png " height="500"></div><div class="title">Figure 1. <a href="https://sign.m2iformation.fr/signin" class="bare">sign.m2iformation.fr/signin</a></div></div></section><section id="_planning_de_formation"><h2>Planning de formation</h2><div class="slide-content"><table class="tableblock frame-all grid-all" style="width:70%"><colgroup><col style="width:50%"><col style="width:25%"><col style="width:25%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Mercredi</strong> 1 Février 2023</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">9:00 à 12:30</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">13:30 à 17:30</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Jeudi</strong> 2 Février 2023</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">9:00 à 12:30</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">13:30 à 17:30</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Vendredi</strong> 3 Février 2023</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">9:00 à 12:30</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">13:30 à 16:00</p></td></tr></table></div></section><section id="_objectif_de_la_formation"><h2>Objectif de la formation</h2><div class="slide-content"><div class="ulist"><ul><li><p>Identifier les caractéristiques principales de <strong>Spark</strong> et de <strong>Databricks</strong></p></li><li><p>Décrire comment les <strong>DataFrames</strong> sont créées et évaluées dans Spark</p></li><li><p>Appliquer les transformations DataFrame pour traiter et analyser les données</p></li></ul></div></div></section><section id="_programme_de_la_formation_en_détail"><h2>Programme de la formation en détail</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://www.m2iformation.fr/formation-spark-avec-databricks/BIG-SPKDAT/" class="bare">www.m2iformation.fr/formation-spark-avec-databricks/BIG-SPKDAT/</a></p></li></ul></div></div></section><section id="_matériel_de_formation"><h2>Matériel de formation</h2><div class="slide-content"><div class="ulist"><ul><li><p>Outils: Azure Databricks, Python, Jupyter Notebooks&#8230;&#8203;</p></li><li><p>Communication: Teams</p></li></ul></div>
<div class="paragraph"><p><span class="image"><img src="https://www.vectorlogo.zone/logos/apache_spark/apache_spark-ar21.svg" alt="apache spark ar21" width="alt" height="200"></span>
<span class="image"><img src="https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png" alt="Databricks Logo" width="alt" height="200"></span>
<span class="image"><img src="https://www.vectorlogo.zone/logos/python/python-official.svg" alt="python official" width="alt" height="200"></span>
<span class="image"><img src="https://www.vectorlogo.zone/logos/numpy/numpy-ar21.svg" alt="numpy ar21" width="alt" height="200"></span>
<span class="image"><img src="https://www.vectorlogo.zone/logos/jupyter/jupyter-ar21.svg" alt="jupyter ar21" width="alt" height="200"></span>
<span class="image"><img src="https://upload.wikimedia.org/wikipedia/commons/c/c9/Microsoft_Office_Teams_%282018%E2%80%93present%29.svg" alt="Microsoft Office Teams %282018%E2%80%93present%29" width="alt" height="200"></span></p></div></div></section><section id="_support_de_formation"><h2>Support de formation</h2><div class="slide-content"><div class="ulist"><ul><li><p>Slides</p></li><li><p>Jupyter Notebooks Python</p></li></ul></div></div></section></section>
<section><section id="_azure_databricks"><h2>Azure Databricks</h2><div class="slide-content"><div class="paragraph"><p><span class="image"><img src="https://upload.wikimedia.org/wikipedia/commons/a/a8/Microsoft_Azure_Logo.svg" alt="Microsoft Azure Logo" width="alt" height="150"></span>
<span class="image"><img src="https://upload.wikimedia.org/wikipedia/commons/6/63/Databricks_Logo.png" alt="Databricks Logo" width="alt" height="200"></span></p></div></div></section><section id="_sommaire_2"><h2>Sommaire</h2><div class="slide-content"><div class="ulist"><ul><li><p>Qu’est-ce que Databricks</p></li><li><p>Présentation de Azure Databricks</p></li><li><p>A quoi sert Azure Databricks</p></li><li><p>Quels use case avec pour Azure Databricks</p></li><li><p>Quels sont les composants de Databricks ?</p></li><li><p>Présentation des espaces de travail de Databricks</p></li><li><p>Quiz</p></li><li><p>Session pratique</p></li></ul></div></div></section></section>
<section><section id="_quest_ce_que_databricks" class="columns is-vcentered"><h2>Qu&#8217;est-ce que Databricks</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p>Plateforme d&#8217;analyse de données qui permet aux entreprises de <strong>traiter</strong>, <strong>stocker</strong> et <strong>analyser</strong> leurs données</p></li><li><p>Permet l&#8217;utilisation des outils de data science et Machine Learning.</p></li><li><p><strong>Databricks</strong> fournit une plateforme <strong>collaborative</strong> pour les <strong>développeurs</strong>, les <strong>data scientists</strong> et les <strong>data engineers</strong> pour <strong>créer</strong>, <strong>déployer</strong> et <strong>gérer</strong> des applications d&#8217;analyse de données</p></li></ul></div></div></div><div class="openblock column"><div class="content"><div class="imageblock"><img src="media/j1/image_2023-01-22-16-51-23.png_.png" alt="image 2023 01 22 16 51 23.png " height="500"></div></div></div></div></section><section id="_quest_ce_que_databricks_2"><h2>Qu&#8217;est-ce que Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p>Il prend en charge de nombreux outils <strong>Open Source</strong> de data science tels que <strong>Spark</strong>, <strong>Delta Lake</strong>, <strong>Redash</strong> et <strong>MLflow</strong></p></li><li><p>Facilite la manipulation de données massives, la construction de modèles Machine Learning (ML) et la mise en œuvre de projets d&#8217;analyse de données.</p></li><li><p>Il est souvent utilisé dans les entreprises pour le traitement de données en temps réel et l&#8217;analytique avancée.</p></li></ul></div></div></section><section id="_azure_databricks_lakehouse"><h2>Azure Databricks Lakehouse</h2><div class="slide-content"><div class="ulist"><ul><li><p>La plateforme Azure Databricks <strong>Lakehouse</strong> fournit un ensemble unifié d&#8217;outils pour <strong>construire</strong>, <strong>déployer</strong>, <strong>partager</strong> et <strong>maintenir</strong> des solutions de données d&#8217;entreprise à grande échelle.</p></li><li><p>Azure Databricks s&#8217;intègre à un stockage et sécurité dans votre <strong>compte cloud</strong>, et <strong>gère</strong> et <strong>déploie</strong> l&#8217;infrastructure cloud en votre nom.</p></li></ul></div></div></section><section id="_trois_utilisateurs_des_données" class="columns"><h2>Trois utilisateurs des données</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><strong>Business Intelligence</strong></p><div class="ulist"><ul><li><p>SQL and BI tools</p></li><li><p>Préparation et exécution des rapports</p></li><li><p>Visualisation des données</p></li><li><p>(parfois) Big data</p></li><li><p>Données structurées</p></li></ul></div></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><strong>Data Science</strong></p><div class="ulist"><ul><li><p>R, SAS, Python</p></li><li><p>Analyses statistiques</p></li><li><p>Explication des données</p></li><li><p>Visualisation des données</p></li></ul></div></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><strong>Machine Learning</strong></p><div class="ulist"><ul><li><p>Python</p></li><li><p>Deep learning</p></li><li><p>modèles prédictives</p></li><li><p>Déploiement des modèles en prod</p></li><li><p>Données non structurées</p></li></ul></div></li></ul></div></div></div></div></section><section id="_data_lake_vs_data_warehouse"><h2>Data Lake vs Data Warehouse</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-21-24-26.png_.png" alt="image 2023 01 22 21 24 26.png " height="700"></div></div></section><section id="_data_lake_vs_data_warehouse_2"><h2>Data Lake vs Data Warehouse</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-16-52-25.png_.png" alt="image 2023 01 22 16 52 25.png " height="200"></div>
<div class="imageblock"><img src="media/j1/image_2023-01-22-21-34-31.png_.png" alt="image 2023 01 22 21 34 31.png " height="500"></div></div></section><section id="_data_lake_vs_data_warehouse_3"><h2>Data Lake vs Data Warehouse</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-16-54-43.png_.png" alt="image 2023 01 22 16 54 43.png " height="500"></div></div></section><section id="_data_lakehouse"><h2>Data LakeHouse</h2><div class="slide-content"><div class="paragraph"><div class="title"><a href="https://www.databricks.com/glossary/data-lakehouse">Data LakeHouse</a> = Data Lake + Data Warehouse</div><p><span class="image"><img src="https://www.databricks.com/wp-content/uploads/2020/01/data-lakehouse-new.png" alt="data lakehouse new" height="700"></span></p></div></div></section><section id="_data_lakehouse_2"><h2>Data LakeHouse</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-21-39-45.png_.png" alt="image 2023 01 22 21 39 45.png " height="700"></div><div class="title">Figure 2. Lakehouse, Databricks Inc</div></div></section></section>
<section><section id="_présentation_de_azure_databricks"><h2>Présentation de Azure Databricks</h2></section><section id="_intégration_gérée_avec_lopen_source"><h2>Intégration gérée avec l&#8217;open source</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Databricks</strong> a un fort engagement envers la communauté <a href="https://www.databricks.com/product/open-source">open source</a>.</p></li><li><p>Les technologies suivantes sont des projets open source fondés par des <strong>employés de Databricks</strong></p><div class="ulist"><ul><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/workflows/">Delta Lake</a></p></li><li><p><a href="https://delta.io/sharing">Delta Sharing</a></p></li><li><p><a href="https://mlflow.org/">MLflow</a></p></li><li><p><a href="https://spark.apache.org/">Apache Spark</a> et <a href="https://spark.apache.org/streaming/">Structured Streaming</a></p></li><li><p><a href="https://redash.io/">Redash</a></p></li></ul></div></li></ul></div></div></section><section id="_intégration_gérée_avec_lopen_source_2"><h2>Intégration gérée avec l&#8217;open source</h2><div class="slide-content"><div class="ulist"><ul><li><p>Azure Databricks maintient un certain nombre d&#8217;outils propriétaires qui intègrent et étendent ces technologies pour ajouter des performances optimisées et une facilité d&#8217;utilisation, tels que les suivants:</p><div class="ulist"><ul><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/workflows/">Workflows</a></p></li><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/data-governance/unity-catalog/">Unity Catalog</a></p></li><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/workflows/delta-live-tables/">Delta Live Tables</a></p></li><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/sql/">Databricks SQL</a></p></li><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/runtime/photon">Photon</a></p></li></ul></div></li></ul></div></div></section><section id="_principaux_composants_de_databricks_lakehouse" class="columns"><h2>Principaux composants de Databricks Lakehouse</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><strong>Tables Delta</strong></p><div class="ulist"><ul><li><p>Transactions <a href="https://www.databricks.com/glossary/acid-transactions">ACID</a> <sup class="footnote">[<span class="footnote" title="View footnote.">1</span>]</sup></p></li><li><p>Contrôle de version des données</p></li><li><p>ETL <sup class="footnote">[<span class="footnote" title="View footnote.">2</span>]</sup></p></li><li><p>Indexation</p></li></ul></div></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><strong>Unity Catalog</strong></p><div class="ulist"><ul><li><p>Gouvernance des données</p></li><li><p>Partage des données</p></li><li><p>Audit des données</p></li></ul></div></li></ul></div></div></div></div><div class="footnotes"><div class="footnote">1. Atomicity, Consistency, Isolation, and Durability</div><div class="footnote">2. extract, transform, et load</div></div></section><section id="_tables_delta"><h2>Tables Delta</h2><div class="slide-content"><div class="ulist"><ul><li><p>Les tables créées sur Azure Databricks utilisent le protocole Delta Lake par défaut</p></li><li><p>Lorsque vous créez une table Delta :</p><div class="ulist"><ul><li><p>Les métadonnées utilisées pour référencer la table sont ajoutées au <strong>metastore</strong> dans le schéma ou la base de données déclaré.</p></li><li><p>Les données et les métadonnées de table sont enregistrées dans un répertoire dans le stockage d’objets cloud.</p></li></ul></div></li></ul></div></div></section><section id="_tables_delta_2"><h2>Tables Delta</h2><div class="slide-content"><div class="ulist"><ul><li><p>Vous pouvez créer des <strong>tables Delta</strong> en interagissant directement avec des chemins d’accès de répertoire à l’aide des API Spark.</p></li><li><p>Certaines nouvelles fonctionnalités qui s’appuient sur <strong>Delta Lake</strong> stockent des métadonnées supplémentaires dans le répertoire de table, mais toutes les tables Delta présentent ce qui suit :</p><div class="ulist"><ul><li><p><strong>Répertoire</strong> contenant des données au format de fichier <strong>Parquet</strong>.</p></li><li><p><strong>Sous-répertoire</strong> <strong>/_delta_log</strong> contenant des <strong>métadonnées</strong> sur les <strong>versions</strong> de table au format <strong>JSON</strong> et <strong>Parquet</strong>.</p></li></ul></div></li></ul></div></div></section><section id="_unity_catalog"><h2>Unity Catalog</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Unity Catalog</strong> unifie la gouvernance et la découverte des données sur Azure Databricks.</p></li><li><p>Disponible dans les notebooks, les Jobs et Databricks SQL, Unity Catalog fournit des fonctionnalités et des interfaces utilisateur qui activent les charges de travail et utilisateurs conçus pour les lacs de données (Data Lake) et l’entrepôt de données (Data warehouse).</p></li><li><p>La gestion au niveau du compte du metastore signifie que les bases de données, les objets de données et les autorisations peuvent être partagés entre les espaces de travail Azure Databricks.</p></li></ul></div></div></section><section id="_unity_catalog_2"><h2>Unity Catalog</h2><div class="slide-content"><div class="ulist"><ul><li><p>Vous pouvez tirer parti d’un espace de noms à trois niveaux (<strong>&lt;catalog&gt;.&lt;database&gt;.&lt;table&gt;</strong>) pour organiser et accorder l’accès aux données.</p></li><li><p>Les emplacements externes et les informations d’identification de stockage sont également des objets sécurisables avec un paramètre ACL similaire à d’autres objets de données.</p></li><li><p><strong>Data Explorer</strong> fournit une interface graphique utilisateur pour explorer les bases de données et gérer les autorisations.</p></li></ul></div></div></section></section>
<section><section id="_a_quoi_sert_azure_databricks"><h2>A quoi sert Azure Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p>Les clients utilisent Azure Databricks pour <code>traiter</code>, <code>stocker</code>, <code>nettoyer</code>, <code>partager</code>, <code>analyser</code>, <code>modéliser</code> et <code>monétiser</code> leurs jeux de données avec des solutions allant du BI au machine learning.</p></li><li><p>Les clients qui adoptent pleinement le <strong>lakehouse</strong> <sup class="footnote">[<span class="footnote" title="View footnote.">1</span>]</sup> profitent d&#8217;une plateforme unifiée pour <code>construire</code> et <code>déployer</code> des flux de données, des <code>modèles Machine Learning</code> et des <code>tableaux de bord analytiques</code> qui alimentent les innovations et les insights dans toute l&#8217;organisation.</p></li></ul></div></div><div class="footnotes"><div class="footnote">1. Lakehouse: Lac de données</div></div></section><section id="_a_quoi_sert_azure_databricks_2"><h2>A quoi sert Azure Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p>Le workspace Azure Databricks fournit des interfaces utilisateur pour de nombreuses tâches de données fondamentales, notamment des outils pour les éléments suivants:</p></li></ul></div>
<table class="tableblock frame-none grid-none" style="width:90%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Notebooks interactifs</p></li><li><p>Éditeur SQL et tableaux de bord</p></li><li><p>Découverte, annotation et exploration de données</p></li><li><p>Suivi des expériences Machine Learning (ML)</p></li><li><p>Un magasin de fonctionnalités</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Planificateur et gestionnaire de flux de travail</p></li><li><p>Ingestion de données et gouvernance</p></li><li><p>Gestion des calculs</p></li><li><p>Mise en service de modèles ML</p></li><li><p>Contrôle de source avec Git</p></li></ul></div></div></td></tr></table></div></section><section id="_a_quoi_sert_azure_databricks_3"><h2>A quoi sert Azure Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p>En plus de l&#8217;interface utilisateur de l&#8217;espace de travail, vous pouvez interagir avec Azure Databricks de manière programmatique avec les outils suivants:</p><div class="ulist"><ul><li><p>API REST</p></li><li><p>CLI</p></li><li><p>Terraform</p></li></ul></div></li></ul></div></div></section></section>
<section><section id="_quels_use_case_avec_pour_azure_databricks"><h2>Quels use case avec pour Azure Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p>Les cas d&#8217;utilisation d&#8217;Azure Databricks sont aussi variés que les données traitées sur la plateforme et les nombreux personnels qui travaillent avec des données comme une partie essentielle de leur travail.</p></li><li><p>Les cas d&#8217;utilisation suivants mettent en évidence comment les utilisateurs peuvent utiliser Azure Databricks pour accomplir des tâches essentielles pour <strong>traiter</strong>, <strong>stocker</strong> et <strong>analyser</strong> les données qui conduisent les fonctions commerciales et les décisions critiques.</p></li></ul></div></div></section><section id="_construire_un_lakehouse_de_lentreprise"><h2>Construire un lakehouse de l&#8217;entreprise</h2><div class="slide-content"><div class="ulist"><ul><li><p>Le <strong>lakehouse</strong> combine les forces des <strong>data warehouse</strong> et des data lake pour <strong>accélérer</strong>, <strong>simplifier</strong> et <strong>unifier</strong> les solutions de données d&#8217;entreprise.</p></li><li><p>Les Data Engineer, les Data Scientist, les data analystes peuvent tous utiliser le <strong>lakehouse</strong> comme une seule <strong>source de vérité</strong>, permettant un accès en temps opportun à des données cohérentes et réduisant les complexités de la construction, de la maintenance et de la synchronisation de nombreux systèmes de données distribués.</p></li></ul></div></div></section><section id="_etl_et_ingénierie_des_données"><h2>ETL et ingénierie des données</h2><div class="slide-content"><div class="ulist"><ul><li><p>Que vous génériez des tableaux de bord ou alimentiez des applications d&#8217;intelligence artificielle, l&#8217;ingénierie des données constitue la colonne vertébrale des entreprises axées sur les données en veillant à ce que les données soient <strong>disponibles</strong>, <strong>propres</strong> et <strong>stockées</strong> dans des modèles de données permettant une découverte et une utilisation efficaces.</p></li><li><p><strong>Azure Databricks</strong> combine la puissance d'<strong>Apache Spark</strong> avec <strong>Delta Lake</strong> et des outils personnalisés pour fournir une expérience <strong>ETL</strong> (extraction, transformation, chargement).</p></li><li><p>Vous pouvez utiliser <strong>SQL</strong>, <strong>Python</strong>, <strong>R</strong> et <strong>Scala</strong> pour composer la logique ETL et orchestrer le déploiement de tâches planifiées en quelques clics.</p></li></ul></div></div></section><section id="_etl_et_ingénierie_des_données_2"><h2>ETL et ingénierie des données</h2><div class="slide-content"><div class="ulist"><ul><li><p>Les <a href="https://learn.microsoft.com/fr-fr/azure/databricks/workflows/delta-live-tables/">Delta Live Table</a> simplifient encore plus l&#8217;ETL en gérant intelligemment les dépendances entre les jeux de données en déployant et en redimensionnant automatiquement l&#8217;infrastructure de production pour garantir une livraison précise des données selon vos spécifications.</p></li><li><p>Azure Databricks propose un certain nombre d&#8217;outils personnalisés pour <a href="https://learn.microsoft.com/fr-fr/azure/databricks/ingestion/">l&#8217;ingestion de données</a>, notamment <a href="https://learn.microsoft.com/fr-fr/azure/databricks/ingestion/auto-loader/">Auto Loader</a>, un outil efficace et évolutif pour charger de manière incrémentielle et idempotente les données à partir de stockage d&#8217;objets cloud et de lacs de données dans le lac de données.</p></li></ul></div></div></section><section id="_machine_learning_ai_et_data_science"><h2>Machine learning, AI, et data science</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Azure Databricks</strong> étend les fonctionnalités de base de la plateforme avec une suite d&#8217;outils adaptés aux besoins des Data Scientist et des ingénieurs en Machine Learning, notamment <a href="https://learn.microsoft.com/fr-fr/azure/databricks/mlflow/">MLflow</a> et le <a href="https://learn.microsoft.com/fr-fr/azure/databricks/runtime/mlruntime">Databricks Runtime for Machine Learning</a>.</p></li></ul></div></div></section><section id="_data_warehousing_analytiques_et_bi"><h2>Data warehousing, analytiques, et BI</h2><div class="slide-content"><div class="ulist"><ul><li><p>Azure Databricks combine des <strong>interfaces utilisateur</strong> conviviales avec des ressources de calcul <strong>économiques</strong> et un <strong>stockage évolutif</strong> et <strong>abordable</strong> pour fournir une plateforme puissante pour exécuter des requêtes analytiques.</p></li><li><p>Les administrateurs configurent des clusters de calcul évolutifs en tant <a href="https://learn.microsoft.com/fr-fr/azure/databricks/sql/admin/sql-endpoints">SQL Warehouse</a>, permettant aux utilisateurs finaux d&#8217;exécuter des requêtes sans se soucier de la complexité de travailler dans le cloud.</p></li><li><p>Les utilisateurs SQL peuvent exécuter des requêtes contre des données dans le lac de données à l&#8217;aide de <a href="https://learn.microsoft.com/fr-fr/azure/databricks/sql/user/queries/queries">SQL Query Editor</a> ou dans des notebooks.</p></li><li><p>Les <a href="https://learn.microsoft.com/fr-fr/azure/databricks/notebooks/">notebooks</a> prennent en charge les languages <strong>Python</strong>, <strong>R</strong> et <strong>Scala</strong> en plus de <strong>SQL</strong> et permettent aux utilisateurs d&#8217;intégrer les mêmes <a href="https://learn.microsoft.com/fr-fr/azure/databricks/notebooks/visualizations/">visualisations</a> disponibles dans les <a href="https://learn.microsoft.com/fr-fr/azure/databricks/sql/user/dashboards/">tableaux de bord</a> avec des liens, des images et des commentaires écrits en markdown.</p></li></ul></div></div></section><section id="_gouvernance_des_données_et_partage_de_données_sécurisé"><h2>Gouvernance des données et partage de données sécurisé</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/data-governance/unity-catalog/">Unity Catalog</a> fournit un <strong>modèle de gouvernance des données</strong> unifié pour le lac de données.</p></li><li><p>Les <strong>administrateurs cloud</strong> configurent et intègrent les autorisations de contrôle d&#8217;accès grossier pour Unity Catalog, et les <strong>administrateurs Azure Databricks</strong> peuvent ensuite gérer les autorisations pour les équipes et les individus.</p></li><li><p><strong>Unity Catalog</strong> rend l&#8217;analyse sécurisée dans le cloud simple et offre une division des responsabilités qui aide à limiter les compétences nécessaires pour les administrateurs et les utilisateurs finaux de la plateforme.</p></li><li><p>Le <strong>Lakehouse</strong> rend le partage de données au sein de votre organisation aussi simple que d&#8217;accorder l&#8217;accès en lecture à une table ou une vue. Pour partager en dehors de votre environnement sécurisé, Unity Catalog dispose d&#8217;une version gérée de <a href="https://learn.microsoft.com/fr-fr/azure/databricks/data-sharing/">Delta Sharing</a>.</p></li></ul></div></div></section><section id="_devops_cicd_et_orchestration_des_tâches"><h2>DevOps, CI/CD et orchestration des tâches</h2><div class="slide-content"><div class="ulist"><ul><li><p>Les cycles de vie de développement pour les <strong>pipelines ETL</strong>, les <strong>modèles ML</strong> et les <strong>tableaux de bord analytiques</strong> présentent chacun leurs propres défis uniques.</p></li><li><p><strong>Azure Databricks</strong> permet aux utilisateurs de tirer parti d&#8217;une <strong>seule source de données</strong>, ce qui réduit les efforts en double et les rapports désynchronisés.</p></li><li><p>En fournissant en outre une suite d&#8217;outils courants pour la <strong>versioning</strong>, l'<strong>automatisation</strong>, la <strong>planification</strong>, le <strong>déploiement</strong> du code et des ressources de production, vous pouvez simplifier votre charge de travail pour la <strong>surveillance</strong>, l'<strong>orchestration</strong> et les <strong>opérations</strong>.</p></li><li><p><strong>Databricks Workflows</strong> planifient les notebooks Azure Databricks, les requêtes SQL et d&#8217;autres codes arbitraires.</p></li><li><p><strong>Databricks Repos</strong> permettent de synchroniser les projets Azure Databricks avec plusieurs fournisseurs git populaires.</p></li></ul></div></div></section><section id="_real_time_et_streaming_analytiques"><h2>Real-time et streaming analytiques</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Azure Databricks</strong> utilise <a href="https://learn.microsoft.com/fr-fr/azure/databricks/structured-streaming/">Apache Spark Structured Streaming</a> pour travailler avec des données en streaming et des modifications de données incrémentielles.</p></li><li><p><strong>Structured Streaming</strong> s&#8217;intègre étroitement avec <strong>Delta Lake</strong>, et ces technologies constituent les fondements à la fois pour les <strong>Delta Live Tables</strong> et pour <strong>Auto Loader</strong></p></li></ul></div></div></section><section id="_real_time_et_streaming_analytiques_données_météo_france"><h2>Real-time et streaming analytiques: Données Météo France</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-18-47-30.png_.png" alt="image 2023 01 22 18 47 30.png " height="700"></div></div></section></section>
<section><section id="_quels_sont_les_composants_de_databricks"><h2>Quels sont les composants de Databricks ?</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Plateforme de données collaborative</strong>: Databricks fournit une plateforme collaborative pour les Data Scientists et les Data Engineer et ingénieurs ML pour créer, déployer et gérer des applications d&#8217;analyse de données.</p></li></ul></div></div></section><section id="_apache_spark" class="columns is-vcentered"><h2>Apache Spark</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p>Databricks est construit sur la plateforme <strong>Apache Spark</strong>, qui est un moteur de <strong>calcul distribué</strong> pour les données en <strong>mémoire</strong>.</p></li><li><p>Il permet de <strong>traiter des données massives à des vitesses élevées</strong>, ce qui est idéal pour les analyses en temps réel.</p></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="imageblock"><img src="https://i.stack.imgur.com/ZZUvJ.png" alt="ZZUvJ" height="700"></div></div></div></div></section><section id="_databricks_vs_apache_spark"><h2>Databricks vs Apache Spark</h2><div class="slide-content"><div class="imageblock"><img src="https://www.databricks.com/wp-content/uploads/2019/04/Marketecture-Spark-comp-page.png" alt="Marketecture Spark comp page" height="700"></div></div></section><section id="_delta_lake" class="columns is-vcentered"><h2>Delta Lake</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><a href="https://delta.io/"><strong>Delta Lake</strong></a> est un système de stockage de données en ligne qui permet de gérer les données de manière <strong>fiable et efficace</strong>.</p></li><li><p>Il prend en charge les <strong>transactions</strong>, les <strong>versions</strong> et les <strong>stratégies</strong> de rétention de données pour garantir la qualité des données.</p></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="imageblock"><img src="https://www.databricks.com/wp-content/uploads/2019/04/Delta-Lake-marketecture-0423c.png" alt="Delta Lake marketecture 0423c" height="700"></div></div></div></div></section><section id="_mlflow" class="columns is-vcentered"><h2>MLflow</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><strong>MLflow</strong>: C&#8217;est un outil open-source qui permet de gérer les workflows Machine Learning de bout en bout, y compris la gestion des modèles, des expériences et des déploiements.</p></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="imageblock"><img src="https://www.databricks.com/wp-content/uploads/2021/02/mlflow-components2.png" alt="mlflow components2" height="300"></div></div></div></div></section><section id="_notebooks" class="columns is-vcentered"><h2>Notebooks</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><strong>Databricks Notebooks</strong> fournit des notebooks intégrés pour <strong>écrire</strong>, <strong>exécuter</strong> et <strong>partager</strong> du code, des requêtes SQL et des visualisations.</p></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="imageblock"><img src="https://docs.databricks.com/_images/side-by-side.gif" alt="side by side"></div></div></div></div></section><section id="_intégration_doutils"><h2>Intégration d&#8217;outils</h2><div class="slide-content"><div class="ulist"><ul><li><p>Databricks permet l'<strong>intégration</strong> avec des outils (e.g., AWS S3, Azure Blob Storage, Google Cloud Storage, etc. ) pour <strong>stocker</strong> et <strong>accéder</strong> aux données.</p></li><li><p>Il prend également en charge les intégrations avec des <strong>outils de BI</strong> (e.g., Tableau, Power BI, etc.) pour les visualisations et les rapports.</p></li></ul></div></div></section><section id="_sécurité_et_conformité"><h2>Sécurité et conformité</h2><div class="slide-content"><div class="ulist"><ul><li><p>Databricks offre des fonctionnalités de <strong>sécurité</strong> et de <strong>conformité</strong> telles que l'<strong>authentification</strong>, l'<strong>autorisation</strong>, la <strong>gestion des accès</strong>, la <strong>chiffrement des données</strong> pour protéger les données sensibles.</p></li></ul></div></div></section></section>
<section><section id="_présentation_des_espaces_de_travail"><h2>Présentation des espaces de travail</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-19-00-10.png_.png" alt="image 2023 01 22 19 00 10.png " height="700"></div></div></section><section id="_databricks_data_science_engineering"><h2>Databricks Data Science &amp; Engineering</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-19-03-33.png_.png" alt="image 2023 01 22 19 03 33.png " height="700"></div></div></section><section id="_databricks_data_science_engineering_2"><h2>Databricks Data Science &amp; Engineering</h2><div class="slide-content"><div class="ulist"><ul><li><p>Databricks Workspace est une plateforme d&#8217;analytique basée sur Apache Spark intégrée à Azure pour faciliter la collaboration entre Data Engineer, Data Scientist et Ingénieurs de Machine Learning.</p></li></ul></div>
<div class="imageblock"><img src="https://learn.microsoft.com/fr-fr/azure/databricks/scenarios/media/what-is-azure-databricks/azure-databricks-overview.png" alt="azure databricks overview" height="500"></div></div></section><section id="_notebooks_2"><h2>Notebooks</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-23-00-16-49.png_.png" alt="image 2023 01 23 00 16 49.png " height="700"></div></div></section><section id="_repos"><h2>Repos</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-23-00-14-13.png_.png" alt="image 2023 01 23 00 14 13.png " height="700"></div><div class="title">Figure 3. <a href="https://github.com/databricks-academy/apache-spark-programming-with-databricks.git">e.g. Apache Spark Programming with Databricks</a></div></div></section><section id="_data"><h2>Data</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-44-45.png_.png" alt="image 2023 01 22 23 44 45.png " height="700"></div></div></section><section id="_compute"><h2>Compute</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-58-18.png_.png" alt="image 2023 01 22 23 58 18.png " height="700"></div></div></section><section id="_workflows"><h2>Workflows</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-56-34.png_.png" alt="image 2023 01 22 23 56 34.png " height="700"></div></div></section><section id="_databricks_machine_learning"><h2>Databricks Machine Learning</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-19-04-26.png_.png" alt="image 2023 01 22 19 04 26.png " height="700"></div></div></section><section id="_databricks_machine_learning_2" class="columns is-vcentered"><h2>Databricks Machine Learning</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p>Databricks Machine Learning est une plateforme de Machine Learning de bout en bout intégré qui incorpore des services managés pour</p><div class="ulist"><ul><li><p>le suivi d’expérimentations,</p></li><li><p>l’entraînement de modèles,</p></li><li><p>le développement et la gestion de fonctionnalités,</p></li><li><p>la fourniture de fonctionnalités et de modèles.</p></li></ul></div></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="imageblock"><img src="https://learn.microsoft.com/fr-fr/azure/databricks/scenarios/media/what-is-azure-databricks/ml-diagram.png" alt="ml diagram" height="400"></div>
<div class="paragraph"><p>Le diagramme montre la correspondance entre les fonctionnalités de Databricks et les étapes du processus de développement et de déploiement de modèle.</p></div></div></div></div></section><section id="_databricks_sql"><h2>Databricks SQL</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-19-05-06.png_.png" alt="image 2023 01 22 19 05 06.png " height="700"></div></div></section><section id="_databricks_sql_2"><h2>Databricks SQL</h2><div class="slide-content"><div class="ulist"><ul><li><p>Databricks SQL décrit l’entrepôt de données d’entreprise intégré à la plateforme Azure Databricks Lakehouse. L’offre principale de Databricks SQL est le calcul optimisé appelé entrepôt SQL.</p></li><li><p>Azure Databricks fournit une collection d’outils d’interface utilisateur connus sous le nom de personnage SQL pour composer et exécuter des requêtes SQL, des visualisations et des tableaux de bord.</p></li><li><p>Les entrepôts SQL fournissent un calcul général pour les requêtes SQL exécutées à partir de nombreux environnements, y compris les outils décisionnels et de visualisation tiers. Databricks SQL fournit également une API robuste.</p></li></ul></div></div></section><section id="_data_2"><h2>Data</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-44-45.png_.png" alt="image 2023 01 22 23 44 45.png " height="700"></div></div></section><section id="_query_editor"><h2>Query Editor</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-29-19.png_.png" alt="image 2023 01 22 23 29 19.png " height="700"></div></div></section><section id="_queries"><h2>Queries</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-40-15.png_.png" alt="image 2023 01 22 23 40 15.png " height="700"></div></div></section><section id="_dashboard"><h2>Dashboard</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-38-36.png_.png" alt="image 2023 01 22 23 38 36.png " height="700"></div></div></section><section id="_alarmes"><h2>Alarmes</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-41-46.png_.png" alt="image 2023 01 22 23 41 46.png " height="700"></div></div></section><section id="_spark_sql_et_databricks_sql"><h2>Spark SQL et Databricks SQL</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Databricks SQL</strong> est un environnement de calcul optimisé, tandis que <strong>Spark SQL</strong> <sup class="footnote">[<span class="footnote" title="View footnote.">1</span>]</sup> décrit une collection d’API Apache Spark.</p></li><li><p>Azure Databricks fournit de nombreux outils et technologies qui permettent aux développeurs SQL d’effectuer des tâches comme l’extraction, la transformation et le chargement (ETL), l’analytique et la création de tableaux de bord. Databricks recommande Databricks SQL pour une expérience d’entrepôt de données d’entreprise.</p></li></ul></div></div><div class="footnotes"><div class="footnote">1. <a href="https://spark.apache.org/sql/" class="bare">spark.apache.org/sql/</a></div></div></section><section id="_databricks_machine_learning_3"><h2>Databricks Machine Learning</h2><div class="slide-content"><div class="ulist"><ul><li><p>Avec Databricks Machine Learning, on peut:</p><div class="ulist"><ul><li><p>Entraîner des modèles manuellement ou avec AutoML.</p></li><li><p>Suivre les paramètres et les modèles d’entraînement en utilisant des expériences avec le suivi <code>MLflow</code>.</p></li><li><p>Créer des tables de fonctionnalités et y accéder pour l’entraînement et l’inférence de modèle.</p></li><li><p>Partager, gérer et traiter des modèles avec le registre de modèles.</p></li><li><p>Pour les applications de Machine Learning, Databricks fournit <code>Databricks Runtime pour Machine Learning</code>, une variante de <code>Databricks Runtime</code> qui inclut de nombreuses bibliothèques de Machine Learning populaires.</p></li></ul></div></li></ul></div></div></section><section id="_fonctionnalités"><h2>Fonctionnalités</h2><div class="slide-content"><div class="paragraph"><p>Feature store, Models, AutoML, Experiments</p></div></div></section><section id="_feature_store"><h2>Feature store</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-19-02.png_.png" alt="image 2023 01 22 23 19 02.png " height="700"></div></div></section><section id="_feature_store_2"><h2>Feature store</h2><div class="slide-content"><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Le <strong>Feature store</strong>  permet de cataloguer les fonctionnalités de ML et de les rendre accessibles pour l’entraînement et le service, en renforçant la réutilisation.</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.databricks.com%2Fwp-content%2Fuploads%2F2021%2F12%2Ffeature-store-img-2-768x549.png&f=1&nofb=1&ipt=c3564e6e6358df1032408cd4342802f303628f2657ea2cda1e97476b7db14348&ipo=images" alt="?u=https%3A%2F%2Fwww.databricks.com%2Fwp content%2Fuploads%2F2021%2F12%2Ffeature store img 2 768x549" height="600"></div></div></td></tr></table></div></section><section id="_experiments"><h2>Experiments</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-20-03.png_.png" alt="image 2023 01 22 23 20 03.png " height="700"></div></div></section><section id="_experiments_2"><h2>Experiments</h2><div class="slide-content"><div class="ulist"><ul><li><p>Les <strong>Experiments</strong> de <code>MLflow</code> permettent</p><div class="ulist"><ul><li><p>de <code>visualiser</code>,</p></li><li><p>de <code>rechercher</code></p></li><li><p>de <code>comparer des exécutions</code>,</p></li><li><p>de <code>télécharger des artefacts</code> et des <code>métadonnées</code> d’exécution à analyser dans d’autres outils.</p></li></ul></div></li><li><p>La page Expériences permet d’accéder rapidement aux <code>expériences MLflow</code></p></li><li><p>Vous pouvez suivre un développement de modèle Machine Learning en  connectant à ces Experiments à partir de notebooks et de travaux Azure Databricks.</p></li></ul></div></div></section><section id="_models"><h2>Models</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-18-12.png_.png" alt="image 2023 01 22 23 18 12.png " height="700"></div></div></section><section id="_models_2"><h2>Models</h2><div class="slide-content"><div class="ulist"><ul><li><p>Azure Databricks héberge un <a href="https://learn.microsoft.com/en-us/azure/databricks/mlflow/model-registry">registre des modèles MLflow</a> pour  permettre de gérer le cycle de vie complet des modèles MLflow.</p></li><li><p>Le registre des modèles assure une <code>traçabilité</code> chronologique des modèles</p><div class="ulist"><ul><li><p>avec indication de l’expérience (Experiment)</p></li><li><p>et de l’exécution MLflow ayant servi à produire le modèle à un moment donné,</p></li><li><p>le versioning des modèles,</p></li><li><p>les transitions de phases (e.g., préproduction à production ou archivage)</p></li><li><p>et des notifications par e-mail des événements de modèle.</p></li></ul></div></li><li><p>Il est aussi possible de créer et afficher des descriptions de modèle et laisser des commentaires.</p></li></ul></div></div></section><section id="_automl"><h2>AutoML</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-23-32.png_.png" alt="image 2023 01 22 23 23 32.png " height="700"></div></div></section><section id="_automl_2"><h2>AutoML</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/machine-learning/automl">AutoML</a>  permet de générer automatiquement des modèles Machine Learning à partir de données et d’accélérer le parcours de production.</p></li><li><p>Trois type de problèmes ML sont possible par l&#8217;AutoML: Classification, prediction, et le forecasting</p></li><li><p>Quatre étapes sont nécessaire pour l&#8217;AutoML: Configuration,
Jointure des Features, Entraînement, Evaluation</p></li><li><p>Il prépare le jeu de données pour l’entraînement du modèle, puis effectue et enregistre un ensemble d’essais, en <code>créant</code>, <code>paramétrant</code> et <code>évaluant</code> plusieurs modèles.</p></li><li><p>Il affiche les résultats et <strong>fournit un notebook</strong> Python avec le code source pour chaque exécution d’essai afin d'`examiner`, <code>reproduire</code> et <code>modifier</code> le code.</p></li><li><p>AutoML <code>calcule</code> également des statistiques récapitulatives sur votre jeu de données et <code>enregistre</code> ces informations dans un notebook que vous pouvez consulter ultérieurement.</p></li></ul></div></div></section><section id="_databricks_runtime_pour_le_machine_learning"><h2>Databricks Runtime pour le machine learning</h2><div class="slide-content"><div class="ulist"><ul><li><p>Databricks Runtime pour Machine Learning (<strong>Databricks Runtime ML</strong>) automatise la création d’un cluster optimisé pour le Machine Learning.</p></li><li><p>Les clusters <strong>Databricks Runtime ML</strong> incluent les bibliothèques de Machine Learning les plus populaires, comme <code>TensorFlow</code>, <code>PyTorch</code>, <code>Keras</code> et <code>XGBoost</code>, ainsi que les bibliothèques nécessaire pour l’entraînement distribué comme <code>Horovod</code>.</p></li><li><p>L’utilisation de <strong>Databricks Runtime ML</strong> <code>accélère</code> la création des clusters et <code>garantit</code> la compatibilité des versions de bibliothèques installées.</p></li></ul></div></div></section><section id="_partner_connect"><h2>Partner Connect</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-49-36.png_.png" alt="image 2023 01 22 23 49 36.png " height="700"></div></div></section><section id="_partner_connect_2"><h2>Partner Connect</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-47-33.png_.png" alt="image 2023 01 22 23 47 33.png " height="700"></div></div></section><section id="_partner_connect_3"><h2>Partner Connect</h2><div class="slide-content"><div class="imageblock"><img src="media/j1/image_2023-01-22-23-48-13.png_.png" alt="image 2023 01 22 23 48 13.png " height="700"></div></div></section></section>
<section id="_quiz"><h2>Quiz</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:33.3333%"><col style="width:33.3333%"><col style="width:33.3334%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Lesquelles des langues sont pris en charge pour créer un notebook sur Databricks?</p><div class="olist arabic"><ol class="arabic"><li><p>Java</p></li><li><p>Python</p></li><li><p>C#</p></li></ol></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Vous voulez entraîner un réseau neuronal avec TensorFlow. Vous ne voulez pas installer la bibliothèque. Que faire?</p><div class="olist arabic"><ol class="arabic"><li><p>Créer un cluster avec le runtime Databricks pour l&#8217;apprentissage automatique.</p></li><li><p>Créer un cluster à noeud unique.</p></li><li><p>Créer un notebook Python.</p></li></ol></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Laquelle de ces descriptions de DBFS est correcte?</p><div class="olist arabic"><ol class="arabic"><li><p>télécharger un fichier sur le DBFS à l&#8217;aide de l&#8217;interface utilisateur.</p></li><li><p>accéder aux données sur Azure Databricks que si elles sont stockées sur DBFS.</p></li><li><p>Les données sur le DBFS ne sont stockées que tant que votre cluster est en d&#8217;exécution.</p></li></ol></div></li></ul></div></div></td></tr></table></div></section>
<section><section id="_session_pratique"><h2>Session pratique</h2><div class="slide-content"><div class="ulist"><ul><li><p>Création de votre premier projet Spark sous Databricks</p></li><li><p>Importer et exporter les données sous Databricks</p></li><li><p>Accéder aux différentes sources de données</p></li><li><p>Créer et manager le cluster</p></li><li><p>Créer un job sous Databricks</p></li><li><p>Explorer les Data set</p></li><li><p>Lancer le Notebook</p></li><li><p>Premier code sous Notebook</p></li><li><p>Exporter le code sous Databricks</p></li></ul></div></div></section><section id="_exercice"><h2>Exercice</h2><div class="slide-content"><div class="paragraph"><p>Bien démarrer avec Azure Databricks</p></div>
<div class="ulist"><ul><li><p>À présent, vous pouvez vous familiariser avec <strong>Azure Databricks</strong> en</p><div class="ulist"><ul><li><p>Configurant un <strong>cluster</strong>,</p></li><li><p>Créant un <strong>espace de travail</strong></p></li><li><p>Créant un <strong>notebook</strong>.</p></li></ul></div></li><li><p>Dans cet exercice, vous allez :</p><div class="ulist"><ul><li><p>Créer un cluster Azure Databricks.</p></li><li><p>Provisionner un espace de travail Azure Databricks.</p></li><li><p>Utiliser des notebooks.</p></li><li><p>Utilisez <strong>DBFS</strong> (Databricks File System).</p></li></ul></div></li></ul></div></div></section><section id="_microsoft_account_azure_pass"><h2>Microsoft Account &#8594; Azure Pass</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:70%"><colgroup><col style="width:60%"><col style="width:40%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="media/j1/image_2023-02-01-19-28-51.png_.png" alt="image 2023 02 01 19 28 51.png "></div><div class="title">Figure 4. <a href="https://www.microsoftazurepass.com/" class="bare">www.microsoftazurepass.com/</a></div>
<div class="imageblock"><img src="media/j1/image_2023-01-24-21-36-05.png_.png" alt="image 2023 01 24 21 36 05.png "></div></div></td><td class="tableblock halign-left valign-top"><div><div class="admonitionblock warning"><table><tr><td class="icon"><i class="fa fa-warning" title="Warning"></i></td><td class="content"><div class="paragraph"><p>Merci d&#8217;utiliser un compte microsoft personnel</p></div></td></tr></table></div></div></td></tr></table></div></section><section id="_souscrire_à_azure_databricks"><h2>Souscrire à Azure Databricks</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="imageblock"><img src="media/j1/image_2023-01-24-11-33-53.png_.png" alt="image 2023 01 24 11 33 53.png " height="700"></div></div></div></div></section><section id="_créer_et_déployez_un_workspace_azure_databricks"><h2>Créer et Déployez un Workspace Azure Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p>Dans le portail Azure <a href="https://portal.azure.com" class="bare">portal.azure.com</a>, créez une nouvelle ressource <a href="https://portal.azure.com"><strong>Azure Databricks</strong></a> en spécifiant les paramètres suivants :</p><div class="ulist"><ul><li><p><strong>Abonnement</strong> : choisissez l&#8217;abonnement Azure dans lequel déployer l&#8217;espace de travail.</p></li></ul></div></li><li><p><strong>Groupe de ressources</strong> : créez un nouveau groupe de ressources.</p></li><li><p><strong>Nom de l&#8217;espace de travail</strong> : fournissez un nom pour votre espace de travail.</p></li><li><p><strong>Région</strong> : sélectionnez un emplacement proche de vous pour le déploiement.</p><div class="ulist"><ul><li><p><strong>Tarif</strong> : Premium</p></li></ul></div></li></ul></div></div></section><section id="_créer_un_cluster" class="columns is-vcentered"><h2>Créer un Cluster</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p>Nom : Entrez un nom unique.</p></li><li><p>Mode de cluster : Noeud unique</p></li><li><p>Version de Databricks Runtime : Sélectionnez l&#8217;édition ML de la dernière version disponible du runtime</p></li><li><p>Ne utilise pas de GPU</p></li><li><p>Inclus Scala &gt; 2.11</p></li><li><p>Inclus Spark &gt; 3.0</p></li><li><p>Terminer après : 120 minutes d&#8217;inactivité</p></li><li><p>Type de nœud : Standard_DS3_v2</p></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="imageblock"><img src="media/j1/image_2023-01-24-22-36-49.png_.png" alt="image 2023 01 24 22 36 49.png " height="700"></div></div></div></div></section><section id="_chargement_des_données_dans_databricks"><h2>Chargement des données dans Databricks</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:65%"><col style="width:35%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Téléchargez le fichier <a href="https://raw.githubusercontent.com/MicrosoftLearning/dp-090-databricks-ml/master/data/nyc-taxi.csv">NYC-Taxi.csv</a> sous le nom <code>nyc-taxi.csv</code> dans n&#8217;importe quel dossier.</p></li><li><p>Sur la page <strong>Data</strong> dans l&#8217;espace de travail Databricks, sélectionnez l&#8217;option  <strong>Créer une Table</strong>.</p></li><li><p>Parcourez jusqu&#8217;au fichier <strong>nyc-taxi.csv</strong> que vous avez téléchargé.</p></li><li><p>Sélectionnez <strong>Créer une table</strong> avec l&#8217;interface utilisateur. Ensuite, sélectionnez votre cluster et sélectionnez Aperçu de la table.</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="media/j1/image_2023-01-24-22-36-19.png_.png" alt="image 2023 01 24 22 36 19.png "></div></div></td></tr></table></div></section><section id="_importer_des_notebooks"><h2>Importer des notebooks</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:65%"><col style="width:35%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Dans l&#8217;espace de travail <strong>Azure Databricks</strong>, en utilisant la barre de commande sur la gauche, sélectionnez l&#8217;espace de travail. Ensuite, sélectionnez Utilisateurs, et ☗ votre nom d&#8217;utilisateur.</p></li><li><p>Sélectionnez <strong>Importer</strong>.</p></li><li><p>Sur la boîte de dialogue Importer des notebooks, importez l&#8217;archive à partir de l'<strong>URL</strong> suivante, en notant qu&#8217;un dossier avec le nom de l&#8217;archive est créé, contenant un ou plusieurs notebooks</p><div class="ulist"><ul><li><p>URL de l&#8217;archive dbc : <a href="https://github.com/MicrosoftLearning/dp-090-databricks-ml/raw/master/01%20-%20Introduction%20to%20Azure%20Databricks.dbc">Introduction to Azure Databricks</a></p></li></ul></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="media/j1/image_2023-01-24-22-34-51.png_.png" alt="image 2023 01 24 22 34 51.png "></div></div></td></tr></table></div></section><section id="_explorer_azure_databricks"><h2>Explorer Azure Databricks</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:65%"><col style="width:35%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Dans cet exercice, vous découvrirez l&#8217;environnement Azure Databricks.</p></li><li><p>Dans le dossier <strong>01 - Introduction to Azure Databricks</strong> de votre espace de travail, ouvrez le notebook <strong>Getting Started with Azure Databricks</strong>.</p></li><li><p>Choisissez votre <strong>cluster</strong> pour attacher votre <strong>notebook</strong> à ce cluster.</p></li><li><p>Lisez les notes dans le notebook, en exécutant chaque cellule de code à tour de rôle.</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="media/j1/image_2023-01-24-22-34-10.png_.png" alt="image 2023 01 24 22 34 10.png "></div></div></td></tr></table></div></section><section id="_nettoyage_de_lenvironnement"><h2>Nettoyage de l&#8217;environnement</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:65%"><col style="width:35%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Si vous avez fini de travailler avec Azure Databricks pour le moment, dans l&#8217;espace de travail Azure Databricks, sur la page <strong>Cluster</strong>,</p><div class="ulist"><ul><li><p>sélectionnez votre cluster et</p></li><li><p>sélectionnez ■ <strong>Terminate</strong> pour l&#8217;arrêter.</p></li></ul></div></li><li><p>Sinon, laissez-le fonctionner pour le prochain exercice.</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="media/j1/image_2023-01-24-22-35-44.png_.png" alt="image 2023 01 24 22 35 44.png "></div></div></td></tr></table></div></section><section id="_exercice_2"><h2>Exercice</h2><div class="slide-content"><div class="ulist"><ul><li><p>Utiliser <strong>Databricks Workflows</strong> pour créer un job sous Databricks</p></li></ul></div></div></section></section>
<section><section id="_apache_spark_2"><h2>Apache Spark</h2><div class="slide-content"><div class="paragraph"><p><span class="image"><img src="https://www.vectorlogo.zone/logos/apache_spark/apache_spark-ar21.svg" alt="apache spark ar21" width="alt" height="200"></span></p></div></div></section><section id="_sommaire_3"><h2>Sommaire</h2><div class="slide-content"><div class="ulist"><ul><li><p>Apache Spark™</p></li><li><p>Architecture de Spark</p></li><li><p>Les composants et les API de Spark</p></li><li><p>Transformations, Actions, et les évaluation lazy</p></li><li><p>Interaction avec Spark</p></li><li><p>Démo de Spark</p></li><li><p>Quiz</p></li><li><p>Session pratique</p></li><li><p>Ressources</p></li></ul></div></div></section></section>
<section><section id="_️_apache_spark"><h2>▶️ Apache Spark™</h2><div class="slide-content"><div class="paragraph"><p><a href="https://spark.apache.org/">Apache Spark™</a> est un moteur multi-langue (Python, SQL, Scala, jJava, R) pour exécuter l&#8217;ingénierie des données, la data science et le machine learning sur des machines mono-nœud ou des clusters.</p></div></div></section><section id="_apache_spark_3"><h2>Apache Spark™</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Spark</strong> offre un stockage en mémoire pour les calculs intermédiaires, ce qui le rend beaucoup plus rapide que <strong>Hadoop MapReduce</strong>.</p></li><li><p>Il incorpore des bibliothèques avec des API combinables</p><div class="ulist"><ul><li><p><strong>MLlib</strong>: le machine learning</p></li><li><p><strong>Spark SQL</strong> : SQL pour les requêtes interactives</p></li><li><p><strong>Structured Streaming</strong> : le traitement des flux pour interagir avec les données en temps réel</p></li><li><p><strong>GraphX</strong> : le traitement de graphes</p></li></ul></div></li></ul></div></div></section><section id="_️_apache_spark_vs_hadoop_map_reduce"><h2>▶️ Apache Spark vs Hadoop Map Reduce</h2><div class="slide-content"><div class="ulist"><ul><li><p>Inconvénient du <code>Hadoop MapReduce</code></p><div class="olist arabic"><ol class="arabic"><li><p>Les résultats sont écrites sue disque (tolérance au pannes) après chaque opération  <code>MapReduce</code> et l&#8217;écriture sur disque est très lente</p></li><li><p>Le jeu d&#8217;expression composé exclusivement de <code>Map</code> et <code>Reduce</code>.e.i., limité, et peu expressif</p></li></ol></div></li><li><p><strong>Apache Spark</strong> est une alternative à <strong>Hadoop MapReduce</strong> pour le calcul distribué qui vise à résoudre ces deux problèmes.</p></li></ul></div></div></section><section id="_apache_spark_vs_hadoop_map_reduce"><h2>Apache Spark vs Hadoop Map Reduce</h2><div class="slide-content"><div class="ulist"><ul><li><p>Les données sur <strong>Spark</strong> sont stockés en mémoire &#8658; 10 à 100 fois plus
de rapidité / Hadoop Map Reduce</p><table class="tableblock frame-none grid-none" style="width:60%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Technologie</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">latence</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">SSD</p></td><td class="tableblock halign-left valign-top"><div><div class="paragraph"><p>1e-4</p></div></div></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">RAM (DDR3)</p></td><td class="tableblock halign-left valign-top"><div><div class="paragraph"><p>1e-8</p></div></div></td></tr></table></li><li><p><strong>Spark</strong> élargit le cadre map/reduce en proposant des opérations distribués.</p></li><li><p>Tout calcul distribué  est réalisé sous la forme d&#8217;opérations <code>Map</code>/<code>Reduce</code></p></li></ul></div></div></section><section id="_apache_spark_vs_hadoop_mapreduce"><h2>Apache Spark vs Hadoop MapReduce</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Hadoop</strong> et <strong>Spark</strong> sont deux framework Big Data mais n’ont pas le même usage</p><div class="ulist"><ul><li><p><strong>Hadoop</strong> est un framework de stockage et de traitement distribué des données</p></li><li><p><strong>Spark</strong> est un framework de traitement des données mais il n’assure pas le stockage</p></li></ul></div></li><li><p>Hadoop et Spark peuvent s’utiliser indépendamment</p></li></ul></div></div></section><section id="_️_historique_de_spark" class="columns is-vcentered"><h2>▶️ Historique de Spark</h2><div class="slide-content"><div class="openblock"><div class="content"><div class="ulist"><ul><li><p>Conçu par <strong>Matei Zaharia</strong> en 2009 lors de sa thèse de doctorat à l’université Californie</p></li><li><p><strong>Spark</strong> est un framework open source de calcul distribué</p></li><li><p>Développé en <strong>Scala</strong></p></li><li><p>La version 1.0 fut lancée en 2014</p></li><li><p>En Juillet 2016, <strong>Apache Spark</strong> est passé en version 2.0</p></li></ul></div></div></div>
<div class="openblock"><div class="content"><div class="paragraph"><p><span class="image"><img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse3.mm.bing.net%2Fth%3Fid%3DOIP.vUU50l-s2XaqQB50PH0sTgHaIo%26pid%3DApi&amp;f=1&amp;ipt=e79a641f420461f169a5b205f497ad4a35db43df9b7d7af03ce592a9456f12fa&amp;ipo=images" alt="?u=https%3A%2F%2Ftse3.mm.bing.net%2Fth%3Fid%3DOIP" height="300"></span></p></div></div></div></div></section><section id="_️_écosystème_spark"><h2>▶️ Écosystème Spark</h2><div class="slide-content"><div class="imageblock"><img src="media/j2/image_2023-01-26-21-52-56.png_.png" alt="image 2023 01 26 21 52 56.png "></div></div></section><section id="_️_quels_sont_les_avantages_de_spark"><h2>▶️ Quels sont les avantages de Spark?</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Vitesse</strong> : permet aux applications dans Hadoop de s&#8217;exécuter 100 fois plus rapidement en mémoire et 10 fois plus rapidement sur disque</p></li><li><p><strong>Facilité d&#8217;utilisation</strong> : permet d&#8217;écrire rapidement des applications en Java, Scala et Python</p></li><li><p><strong>Analyse avancée</strong> : prend en charge les requêtes SQL, les données en streaming et l&#8217;analyse avancée.</p></li></ul></div></div></section><section id="_️_caractéristique_de_spark" class="columns"><h2>▶️ Caractéristique de Spark</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><strong>Rapide</strong>: <strong>Spark</strong> est 100 fois plus rapide que <strong>Hadoop</strong></p></li><li><p><strong>Générique</strong></p><div class="ulist"><ul><li><p>Combine SQL, streaming et analyses complexes.</p></li><li><p>Librairies : <strong>SQL</strong> et <strong>DataFrames</strong>, <strong>MLlib</strong>, <strong>GraphX</strong> et <strong>Spark Streaming</strong></p></li></ul></div></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="ulist"><ul><li><p><strong>Facile à utiliser</strong></p><div class="listingblock"><div class="title">Python</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">df = spark.read.json("logs.json")
df.where("age &gt;21").select("name.first").show()</code></pre></div></div></li><li><p><strong>Fonctionne partout</strong></p><div class="ulist"><ul><li><p>S&#8217;exécute sur <code>Hadoop</code>, <code>Apache Mesos</code>, <code>Kubernetes</code>, de manière autonome ou dans le cloud.</p></li><li><p>Language de programmation: <code>Python</code>, <code>Java</code>, <code>Scala</code></p></li></ul></div></li></ul></div></div></div></div></section><section id="_️_vue_densemble"><h2>▶️ Vue d’ensemble</h2><div class="slide-content"><div class="ulist"><ul><li><p>Les <strong>clusters</strong> Apache Spark sont des groupes d’ordinateurs qui sont traités comme un seul ordinateur et gèrent l’exécution des commandes émises à partir des notebooks.</p></li><li><p>Les clusters permettent de traiter des données sur plusieurs ordinateurs pour améliorer la mise à l’échelle et les performances.</p></li><li><p>Ils se composent de <strong>nœuds</strong> Spark <strong>Driver</strong> et <strong>Worker</strong>.</p><div class="ulist"><ul><li><p>Le <strong>nœud driver</strong> envoie un travail aux <strong>nœuds worker</strong>* et leur demande d’extraire des données d’une source de données spécifiée.</p></li></ul></div></li></ul></div>
<div class="imageblock"><img src="https://learn.microsoft.com/fr-fr/training/wwl-data-ai/use-apache-spark-azure-databricks/media/apache-spark-physical-cluster.png" alt="apache spark physical cluster"></div></div></section><section id="_vue_densemble"><h2>Vue d’ensemble</h2><div class="slide-content"><div class="ulist"><ul><li><p>Dans <strong>Databricks</strong>, l’interface du notebook est généralement le programme <strong>Driver</strong>.</p></li><li><p>Ce programme contient la boucle principale du programme et crée des jeux de données distribués sur le cluster, puis applique des opérations à ces jeux de données.</p></li><li><p>Les programmes Driver accèdent à Apache Spark via un objet <strong>SparkSession</strong>, quel que soit l’emplacement de déploiement.</p></li><li><p><strong>Microsoft Azure</strong> gère le cluster et le met automatiquement à l’échelle selon les besoins, c’est-à-dire en fonction de votre utilisation et du paramètre utilisé lors de la configuration du cluster.</p></li><li><p>L’arrêt automatique peut également être activé, ce qui permet à Azure de mettre fin au cluster après un certain nombre de minutes d’inactivité.</p></li></ul></div></div></section><section id="_vue_densemble_2"><h2>Vue d’ensemble</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:60%"><col style="width:40%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p><strong>Spark</strong> se connecte à une grande variété de sources des données, notamment :</p><div class="ulist"><ul><li><p>Bases de données traditionnelles comme <code>Postgres</code>, <code>SQL Server</code> et <code>MySQL</code></p></li><li><p>Les courtiers de messages comme <code>Kafka</code> et <code>Kinesis</code></p></li><li><p>Bases de données distribuées comme <code>Cassandra</code> et <code>Redshift</code></p></li><li><p>Entrepôts de données comme <code>Hive</code></p></li><li><p>Types de fichiers tels que <code>CSV</code>, <code>Parquet</code> et <code>Avro</code></p></li></ul></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="media/spark_databricks.png" alt="spark databricks"></div></div></td></tr></table></div></section><section id="_️_use_cases_de_spark"><h2>▶️ Use cases de Spark</h2><div class="slide-content"><div class="ulist"><ul><li><p>Traitement en parallèle de grands ensembles de données répartis sur un cluster</p></li><li><p>Effectuer des requêtes interactives pour explorer et visualiser des ensembles de données</p></li><li><p>Construction, entraînement et évaluation de modèles de Machine Learning en utilisant MLlib</p></li><li><p>Mise en place de pipelines de données bout à bout à partir de multiples flux de données</p></li><li><p>Analyse des ensembles de données de graphes et de réseaux sociaux</p></li></ul></div></div></section></section>
<section><section id="_architecture_de_spark"><h2>Architecture de Spark</h2><div class="slide-content"><div class="imageblock"><img src="media/j2/image_2023-01-27-00-05-42.png_.png" alt="image 2023 01 27 00 05 42.png "></div><div class="paragraph"><p><a href="https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html">spark session</a></p></div></div></section><section id="_️_architecture"><h2>▶️ Architecture</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>Spark</strong> se base sur une architecture  <strong>maître / esclave</strong>.</p></li><li><p>Une machine maître <strong>driver</strong> écoute les demandes de traitement clientes, découpe chaque traitement et, délègue sa réalisation à des machines esclaves.</p></li><li><p>C&#8217;est le <strong>driver</strong> qui exécute la méthode <code>main</code> de nos applications.</p></li><li><p>Les machines esclaves <strong>workers</strong> vont accomplir le travail demandé en parallèle. Chaque <strong>worker</strong> instancié un <code>executor</code></p></li><li><p>un <strong>Cluster Manager</strong> est chargé d&#8217;instancier les différents workers</p></li></ul></div></div></section><section id="_️_spark_driver"><h2>▶️ Spark Driver</h2><div class="slide-content"><div class="ulist"><ul><li><p>En tant que partie de l&#8217;application Spark responsable de l&#8217;instanciation d&#8217;une <strong>Spark Session</strong>, le <strong>Spark Driver</strong> a plusieurs rôles :</p><div class="ulist"><ul><li><p>Il communique avec le cluster manager</p></li><li><p>Il demande des ressources (CPU, mémoire, etc.) au cluster manager pour les exécuteurs Spark (JVMs)</p></li><li><p>Il transforme toutes les opérations Spark en calculs DAG, les planifie et distribue leur exécution en tant que tâches à travers les exécuteurs Spark</p></li><li><p>Une fois les ressources allouées, il communique directement avec les exécuteurs.</p></li></ul></div></li></ul></div></div></section><section id="_️_sparksession"><h2>▶️ SparkSession</h2><div class="slide-content"><div class="ulist"><ul><li><p>Dans Spark 2.0, <strong>SparkSession</strong> est devenu un conduit unifié pour toutes les opérations et les données de Spark.</p></li><li><p>Il a non seulement remplacé les points d&#8217;entrée précédents de Spark tels que <strong>SparkContext</strong>, <strong>SQLContext</strong>, <strong>HiveContext</strong>, <strong>SparkConf</strong> et <strong>StreamingContext</strong></p></li><li><p>Il a également rendu le travail avec Spark plus simple et plus facile.</p></li></ul></div></div></section><section id="_️_cluster_manager"><h2>▶️ Cluster manager</h2><div class="slide-content"><div class="ulist"><ul><li><p>Le gestionnaire de cluster est responsable de la <strong>gestion</strong> et de l'<strong>allocation des ressources</strong> pour le cluster de nœuds sur lesquels les applications Spark s&#8217;exécute.</p></li><li><p>Actuellement, Spark prend en charge quatre gestionnaires de cluster :</p><div class="ulist"><ul><li><p>Le gestionnaire de cluster autonome intégré</p></li><li><p>Apache Hadoop YARN</p></li><li><p>Apache Mesos</p></li><li><p>Kubernetes.</p></li></ul></div></li></ul></div></div></section><section id="_️_spark_executor"><h2>▶️ Spark Executor</h2><div class="slide-content"><div class="ulist"><ul><li><p>Un exécuteur Spark s&#8217;exécute sur chaque nœud <strong>worker</strong> dans le cluster.</p></li><li><p>Les exécuteurs communiquent avec le <strong>driver</strong>  et sont responsables de l&#8217;exécution des tâches sur les <strong>workers</strong>.</p></li><li><p>Dans la plupart des modes de déploiement, un seul exécuteur s&#8217;exécute par nœud.</p></li></ul></div></div></section></section>
<section><section id="_les_composants_et_les_api_de_spark"><h2>Les composants et les API de Spark</h2><div class="slide-content"><div class="imageblock"><img src="media/j2/image_2023-01-26-22-19-35.png_.png" alt="image 2023 01 26 22 19 35.png "></div></div></section><section id="_️_spark_core"><h2>▶️ Spark Core</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:60%"><col style="width:40%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p><a href="https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html">Spark core</a> contient les fonctionnalités cœurs pour <strong>Spark</strong></p><div class="ulist"><ul><li><p>Task Scheduling</p></li><li><p>Memory Management</p></li><li><p>Fault Recovery</p></li><li><p>Interaction avec les différents types de stockages et types de données</p></li><li><p>les API de manipulation de RDD en <code>Scala</code> / <code>Python</code> / <code>Java</code></p></li></ul></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="paragraph"><p><span class="image"><img src="media/spark_core.png" alt="spark core"></span></p></div></div></td></tr></table></div></section><section id="_️_spark_sql_et_dataframe"><h2>▶️ Spark SQL et DataFrame</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL et Dataframe</a> fonctionne bien avec les données structurées.</p></li><li><p>Vous pouvez lire les données à partir de formats de fichiers avec des données structurées (CSV, texte, JSON, Avro, ORC, Parquet, etc.)</p></li><li><p>Permet de construire des tables permanentes ou temporaires dans Spark.</p></li><li><p>En utilisant les API Structurées de Spark (en Java, Python, Scala ou R), vous pouvez combiner des requêtes SQL-like pour interroger les données lues dans un DataFrame Spark.</p></li></ul></div></div></section><section id="_spark_sql_et_dataframe"><h2>Spark SQL et DataFrame</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:40%"><col style="width:60%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Spark SQL est un module de traitement de données structurées avec plusieurs interfaces.</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Nous pouvons interagir avec Spark SQL de deux manières :</p><div class="ulist"><ul><li><p>Exécuter des requêtes SQL</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-sql hljs" data-noescape="true" data-lang="sql">%sql
SELECT name, price
FROM products
WHERE price &lt; 200
ORDER BY price</code></pre></div></div></li><li><p>Travailler avec l&#8217;API DataFrame.</p><div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">spark.table("products")
  .select("name", "price")
  .where("price &lt; 200")
  .orderBy("price")</code></pre></div></div></li></ul></div></li></ul></div></div></td></tr></table></div></section><section id="_spark_sql_et_dataframe_conversion"><h2>Spark SQL et DataFrame: Conversion</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:95%"><colgroup><col style="width:60%"><col style="width:40%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p><strong>createOrReplaceTempView</strong> crée une vue temporaire basée sur le DataFrame.</p></li><li><p>La durée de vie de la vue temporaire est liée à la <strong>SparkSession</strong> qui a été utilisée pour créer le DataFrame.</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="listingblock"><div class="title">DataFrame vers SQL</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">df.createOrReplaceTempView("budget")</code></pre></div></div>
<div class="listingblock"><div class="title">SQL vers DataFrame</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">spark.sql("SELECT * FROM budget")
# or
spark.table("budget")</code></pre></div></div></div></td></tr></table></div></section><section id="_exemple_des_opérateurs_dataframe_et_column"><h2>Exemple des opérateurs DataFrame et Column</h2><div class="slide-content"><div class="listingblock"><div class="title">Opérateurs DataFrame et Column</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">revDF = (eventsDF
         .filter(col("ecommerce.purchase_revenue_in_usd").isNotNull())
         .withColumn("purchase_revenue", (col("ecommerce.purchase_revenue_in_usd") * 100).cast("int"))
         .withColumn("avg_purchase_revenue", col("ecommerce.purchase_revenue_in_usd") / col("ecommerce.total_item_quantity"))
         .sort(col("avg_purchase_revenue").desc())
        )</code></pre></div></div></div></section><section id="_api_pandas_de_spark"><h2>API pandas de Spark</h2><div class="slide-content"><div class="ulist"><ul><li><p>L&#8217;API pandas de Spark permet de mettre à l&#8217;échelle la charge de travail pandas</p><div class="ulist"><ul><li><p>Être <strong>immédiatement productif</strong> avec <strong>Spark</strong>, si vous êtes déjà familiarisé avec <strong>pandas</strong>.</p></li><li><p>Avoir <strong>un seul code</strong> qui fonctionne à la fois avec pandas (tests, petits jeux de données) et avec Spark (jeux de données distribués).</p></li><li><p>Passer facilement d&#8217;un contexte de l'<strong>API pandas</strong> et de l'<strong>API PySpark</strong> sans aucun surcoût.</p></li></ul></div></li></ul></div>
<div class="listingblock"><div class="title">Créer un Dataframe Pyspark à partir d&#8217;un Dataframe pandas</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">pandas_df = pd.DataFrame({
    'a': [1, 2, 3],
    'b': [2., 3., 4.],
    'c': ['string1', 'string2', 'string3'],
    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],
    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]
})
df = spark.createDataFrame(pandas_df)
df</code></pre></div></div></div></section><section id="_fonction_définie_par_lutilisateur_udf"><h2>Fonction définie par l&#8217;utilisateur (UDF)</h2><div class="slide-content"><div class="listingblock"><div class="title">Exemple de création et application d&#8217;une UDF</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python"># création d'une fonction python
def firstLetterFunction(email):
    return email[0]

# creation d'une udf
firstLetterUDF = udf(firstLetterFunction)

# application de l'udf
from pyspark.sql.functions import col
display(salesDF.select(firstLetterUDF(col("email"))))</code></pre></div></div></div></section><section id="_️_spark_mllib"><h2>▶️ Spark MLlib</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:65%"><col style="width:35%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>MLlib fournit des algorithmes ML construits sur des API basées sur DataFrame (Classification, régression, clustering, filtrage collaboratif,  ACP).</p></li><li><p>Ces API vous permettent d'<strong>extraire</strong> ou de <strong>transformer</strong> des caractéristiques, de <strong>construire</strong> des pipelines (pour l&#8217;apprentissage et l&#8217;évaluation)</p></li><li><p>MLlib comprend d&#8217;autres primitives ML de bas niveau, notamment la descente de gradient.</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="listingblock"><div class="title">Regression logistique</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">from pyspark.ml.classification import LogisticRegression
...
training = spark.read.csv("s3://...")
test = spark.read.csv("s3://...")
# Load training data
lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)
# Fit the model
lrModel = lr.fit(training)
# Predict
lrModel.transform(test)</code></pre></div></div></div></td></tr></table></div></section><section id="_️_spark_structured_streaming"><h2>▶️ Spark Structured Streaming</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:65%"><col style="width:35%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p><strong>Spark Streaming</strong> est utilisé pour le traitement en <strong>temps-réel des données</strong> en flux.</p><div class="ulist"><ul><li><p>Il s’appuie sur un mode de traitement en "micro batch“</p></li><li><p>Il utilise pour les données temps-réel <code>DStream</code>, c’est-à-dire une série de <code>RDD</code> (Resilient Distributed Dataset).</p></li></ul></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="listingblock"><div class="title">Lecture du stream</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">lines = (spark
.readStream
.format("socket")
.option("host", "localhost")
.option("port", 9999)
.load())</code></pre></div></div></div></td></tr></table></div></section><section id="_️_graphx"><h2>▶️ GraphX</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:65%"><col style="width:35%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p><strong>GraphX</strong> est une bibliothèque pour manipuler des graphes (par exemple, des graphes de réseaux sociaux, des routes et des points de connexion, ou des graphes de topologie de réseau)</p></li><li><p>Elle permet d&#8217;effectuer des calculs parallèles de graphes.</p></li><li><p>Il offre des algorithmes de graphes standard pour l&#8217;analyse, les connexions et les parcours, contribués par les utilisateurs de la communauté.</p></li><li><p>Les algorithmes disponibles incluent PageRank, Composants Connectés et Comptage des Triangles.</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="paragraph"><p><span class="image"><img src="media/spark_core.png" alt="spark core" height="300"></span></p></div></div></td></tr></table></div></section></section>
<section><section id="_transformations_actions_et_les_évaluation_lazy"><h2>Transformations, Actions, et les évaluation lazy</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:65%"><col style="width:35%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Les opérations de Spark sur les données distribuées peuvent être classées en deux types : les <strong>transformations</strong> et les <strong>actions</strong>.</p><div class="ulist"><ul><li><p>Les <strong>transformations</strong> transforment un <strong>DataFrame Spark</strong> en un nouveau DataFrame <strong>sans altérer les données originales</strong>, leur donnant la propriété d&#8217;immuabilité. (e.g., <code>select</code>() ou <code>filter</code>())</p></li><li><p>Une <strong>action</strong> déclenche l&#8217;évaluation paresseuse de toutes les transformations enregistrées.</p></li></ul></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="imageblock"><img src="media/j2/image_2023-01-27-00-54-22.png_.png" alt="image 2023 01 27 00 54 22.png "></div></div></td></tr></table></div></section><section id="_opérateurs_et_méthodes_de_colonne"><h2>Opérateurs et méthodes de colonne</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:90%"><colgroup><col style="width:30%"><col style="width:70%"></colgroup><thead><tr><th class="tableblock halign-left valign-top">*, + , &lt;, &gt;=</th><th class="tableblock halign-left valign-top">Opérateurs mathématiques et de comparaison</th></tr><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">==, !=</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Tests d’égalité et d’inégalité (les opérateurs Scala sont
<code>===</code> et <code>=!=</code>)</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">alias</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Donne à la colonne un alias</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">fonte, astype</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Convertit la colonne en un type de données différent</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">isNull, isNotNull, isNan</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Est nul, n’est pas nul, est NaN</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">asc, desc</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Renvoie une expression de tri basée sur l’ordre
croissant/décroissant de la colonne</p></td></tr></table></div></section><section id="_méthodes_de_transformation_dataframe"><h2>Méthodes de transformation DataFrame</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:90%"><colgroup><col style="width:30%"><col style="width:70%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">select</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Retourne un nouveau DataFrame en calculant l&#8217;expression donnée pour chaque
élément</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">drop</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Renvoie un nouveau DataFrame avec une colonne supprimée</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">withColumnRenamed</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Retourne un nouveau DataFrame avec une colonne renommée</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">withColumn</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Retourne un nouveau DataFrame en ajoutant une colonne ou en remplaçant le
colonne existante portant le même nom</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">filtre, where</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Filtre les lignes en utilisant la condition donnée</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">sort, orderBy</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Retourne un nouveau DataFrame trié par les expressions données</p></td></tr></table></div></section><section id="_méthodes_de_transformation_dataframe_2"><h2>Méthodes de transformation DataFrame</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:90%"><colgroup><col style="width:30%"><col style="width:70%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">dropDuplicates, distinct</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Retourne un nouveau DataFrame avec des lignes en double
supprimé</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">limit</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Retourne un nouveau DataFrame en prenant les n premières lignes</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">groupBy</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">Groupe le DataFrame en utilisant les colonnes spécifiées, afin que nous puissions
exécuter l&#8217;agrégation sur eux</p></td></tr></table>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">&gt;&gt;&gt; strings = spark.read.text("../README.md")
&gt;&gt;&gt; filtered = strings.filter(strings.value.contains("Spark"))
&gt;&gt;&gt; filtered.count()
20</code></pre></div></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">col("ecommerce.purchase_revenue_in_usd") + col("ecommerce.total_item_quantity")
col("event_timestamp").desc()
(col("ecommerce.purchase_revenue_in_usd") * 100).cast("int")</code></pre></div></div></div></section><section id="_rdd"><h2>RDD</h2><div class="slide-content"><div class="ulist"><ul><li><p><strong>RDD</strong>  <sup class="footnote">[<span class="footnote" title="View footnote.">1</span>]</sup>  est le concept central du framework Spark</p><div class="ulist"><ul><li><p><strong>Resilient</strong>: En cas de perte d’un nœud, le sous-traitement sera automatiquement relancé sur un autre nœud</p></li><li><p><strong>Distributed</strong>: jeu de données partitionné et chacune des partitions traitée sur un nœud du cluster</p></li><li><p><strong>Dataset</strong>:  jeu de données qui se parcourt comme une collection</p></li></ul></div></li></ul></div></div><div class="footnotes"><div class="footnote">1. <a href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.html#pyspark.RDD">[Resilient Distributed Dataset</a></div></div></section><section id="_transformation_et_action" class="columns"><h2>Transformation et Action</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p>La RDD permet de manipuler les données avec Spark et supporte deux types d’opérations</p><div class="ulist"><ul><li><p><strong>Les transformations</strong></p></li><li><p><strong>Les actions</strong></p></li></ul></div></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="paragraph"><p><span class="image"><img src="media/rdd.png" alt="rdd"></span></p></div></div></div></div></section><section id="_rdd_transformations" class="columns"><h2>RDD : Transformations</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="ulist"><ul><li><p>Les <strong>transformations</strong> ne retournent pas de résultat, elles retournent un nouveau <strong>RDD</strong></p><div class="ulist"><ul><li><p>Rien n’est évalué lorsque l’on fait appel à une fonction de <strong>transformation</strong> dite “lazy” (e.i., elles ne s’exécuteront que si une opération finale est réalisée en bout de chaîne)</p></li></ul></div></li><li><p>Cette fonction prend juste un RDD et retourne un nouveau RDD.</p></li></ul></div></div></div></div></section><section id="_rdd_transformations_2" class="columns"><h2>RDD : Transformations</h2><div class="slide-content"><div class="openblock column is-two-thirds"><div class="content"><div class="ulist"><ul><li><p>Les principales fonctions de <strong>transformation</strong> sont:</p><div class="ulist"><ul><li><p><strong>map()</strong> permet de transformer un élément en un autre élément</p></li><li><p><strong>mapToPair</strong>() permet de transformer un élément en un tuple clé-valeur</p></li><li><p><strong>filter</strong>() permet de filtrer les éléments en ne conservant que ceux qui correspondent à une expression</p></li><li><p><strong>flatMap</strong>() permet de découper un élément en plusieurs autres éléments</p></li><li><p><strong>reduceByKey</strong>() permet d’agréger des éléments entre eux</p></li><li><p>&#8230;&#8203;</p></li></ul></div></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="paragraph"><p><span class="image"><img src="media/rdd.png" alt="rdd" height="300"></span></p></div></div></div></div></section><section id="_rdd_2" class="columns"><h2>RDD</h2><div class="slide-content"><div class="openblock column is-two-thirds"><div class="content"><div class="ulist"><ul><li><p>Les <strong>actions</strong>: évaluent et retournent une nouvelle valeur.</p><div class="ulist"><ul><li><p>Au moment où une fonction d’action est appelée sur un objet RDD, les transformations nécessaires à l’évaluation de l’action sont calculées</p></li></ul></div></li><li><p>Les  principales actions sont</p><div class="ulist"><ul><li><p><code>count()</code> et <code>countByKey()</code> permettent de compter les éléments</p></li><li><p><code>collect()</code> permet de récupérer les éléments dans une collection</p></li><li><p><code>saveAsTextFile()</code> permet de sauver le résultat dans des fichiers texte</p></li><li><p><code>reduce()</code> permet d’agréger des éléments entre eux</p></li></ul></div></li></ul></div></div></div>
<div class="openblock column"><div class="content"><div class="ulist"><ul><li><p>Un résultat intermédiaire peut être conservé temporairement grâce aux méthodes</p><div class="ulist"><ul><li><p><code>cache()</code> : stockage en mémoire</p></li><li><p><code>persist()</code> : stockage en mémoire ou sur disque</p></li></ul></div></li></ul></div>
<div class="paragraph"><p><span class="image"><img src="media/rdd.png" alt="rdd" height="300"></span></p></div></div></div></div></section><section id="_transformation_vs_action" class="columns"><h2>Transformation vs Action</h2><div class="slide-content"><div class="ulist column"><ul><li><p>Une <strong>transformation</strong> est une opération Spark qui renvoie un DataFrame, un Dataset ou un RDD, exécutée de manière paresseuse.</p></li></ul></div>
<div class="ulist column"><ul><li><p>Les <strong>actions</strong>, quant à elles, ne sont pas exécutées de manière paresseuse et produisent une valeur en effectuant toutes les transformations nécessaires. On peut utiliser les fonctions <code>persist</code> ou <code>cache</code> pour éviter de recalculer les valeurs.</p></li></ul></div></div></section><section id="_rdd_sources_des_données"><h2>RDD : Sources des données</h2><div class="slide-content"><div class="ulist"><ul><li><p>Pour créer une RDD, on peut charger les données à partir de plusieurs sources</p></li><li><p>Un <strong>fichier local</strong> ou distribué (HDFS) dont le format est configurable: texte brut, SequenceFile Hadoop, JSON, etc.</p></li><li><p>Une <strong>base de données</strong>: JDBC, Cassandra, Hbase, etc.</p><div class="ulist"><ul><li><p>Une <strong>collection</strong> (List, Set), transformée en RDD avec l’opérateur parallelize</p></li><li><p>Un autre <strong>RDD</strong> auquel on aura appliqué une transformation
Etc.</p></li></ul></div></li></ul></div>
<div class="admonitionblock note"><table><tr><td class="icon"><i class="fa fa-info-circle" title="Note"></i></td><td class="content"><div class="paragraph"><p>Le chemin inverse, exporter un RDD dans un fichier, dans une base de données ou une collection est aussi possible</p></div></td></tr></table></div></div></section><section id="_lecture_de_données"><h2>Lecture de données</h2><div class="slide-content"><div class="ulist"><ul><li><p>La lecture de données est la responsabilité du <strong>SparkContext</strong></p><div class="ulist"><ul><li><p>Le <strong>SparkContext</strong> produit en sortie un <code>RDD</code></p></li><li><p><code><strong>sc.textFile(path)</strong></code> <sup class="footnote">[<span class="footnote" title="View footnote.">1</span>]</sup> : l&#8217;appel à cette méthode crée un RDD à partir des lignes du fichier. A noter que le <code>path</code> du fichier peut être un chemin réseau, ce qui nous sera utile par la suite.</p></li><li><p><code><strong>sc.wholeTextFiles(path)</strong></code> <sup class="footnote">[<span class="footnote" title="View footnote.">2</span>]</sup> : <code>path</code> peut désigner un chemin vers un fichier ou un répertoire contenant plusieurs fichiers. Le RDD construit est un ensemble de clés-valeurs ou chaque clé indique le chemin vers un fichier et les valeurs sont le contenu complet des fichiers. Le(s) fichier(s) ne sont donc pas décomposés en lignes.</p></li><li><p><code><strong>sc.parallelize(iterable)</strong></code> <sup class="footnote">[<span class="footnote" title="View footnote.">3</span>]</sup> : utilisé pour transformer des objets <code>Python</code> (ou <code>Scala</code>) en RDD.paralléliser est principalement utilisé dans le Spark Shell pour déboguer des applications.</p></li></ul></div></li></ul></div></div><div class="footnotes"><div class="footnote">1. <a href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.SparkContext.textFile.html#pyspark.SparkContext.textFile:">Doc: textFile </a></div><div class="footnote">2. <a href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.SparkContext.wholeTextFiles.html#pyspark.SparkContext.wholeTextFiles:">Doc: wholeTextFiles</a></div><div class="footnote">3. <a href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.SparkContext.parallelize.html#pyspark.SparkContext.parallelize:">Doc: parallelize</a></div></div></section><section id="_dataframe" class="columns"><h2>DataFrame</h2><div class="slide-content"><div class="ulist column"><ul><li><p><code>DataFrame</code>: collection de données distribuées, organisées en colonnes.</p><div class="ulist"><ul><li><p><strong>DataFrame = RDD + colonnes nommées</strong></p></li></ul></div></li><li><p>Similaire à une table dans le monde des bases de données relationnelles.</p></li><li><p>Peut être converties en RDD en appelant la méthode <code>rdd</code>, qui retourne le contenu de la DataFrame sous forme de RDD de lignes</p></li><li><p>L’API Dataset est disponible pour <code>Scala</code>, <code>Java</code>, <code>Python</code> et R.</p></li></ul></div>
<div class="ulist column"><ul><li><p>Peut être crée à partir de différentes sources de données :</p><div class="ulist"><ul><li><p>RDD existants</p></li><li><p>Fichiers de données structurées</p></li><li><p>Jeux de données JSON</p></li><li><p>Tables HIVE</p></li><li><p>Bases de données externes</p></li></ul></div></li></ul></div></div></section><section id="_dataframe_et_column"><h2>DataFrame et Column</h2><div class="slide-content"><div class="ulist"><ul><li><p>Un <strong>DataFrame</strong> est une collection distribuée de données regroupées dans des colonnes nommées.</p><div class="ulist"><ul><li><p>opérations: select, selectExpr, drop, withColumn, withColumnRenamed, filter, distinct, limit, sort</p></li></ul></div></li><li><p>Une <strong>colonne</strong> est une construction logique qui sera calculée sur la base des données d&#8217;un DataFrame à l&#8217;aide d&#8217;une expression</p><div class="ulist"><ul><li><p>opérations: alias, isin, cast, isNotNull, desc, operators</p></li></ul></div></li></ul></div></div></section><section id="_️_pyspark"><h2>▶️ PySpark</h2><div class="slide-content"><div class="ulist"><ul><li><p>PySpark est une interface pour <code>Apache Spark</code> en <code>Python</code></p><div class="ulist"><ul><li><p>Lien entre l’API python et Spark Core</p></li><li><p><code>Python</code> &gt; 3.6</p></li><li><p>Disponible depuis PiP ou Conda (pip install pyspark)</p></li></ul></div></li><li><p>Dépendance</p><div class="ulist"><ul><li><p>Pandas  : 0.23.2 (Optionnel pour SQL)</p></li><li><p>NumPy  : 1.7 (Requis pour ML)</p></li><li><p>Pyarrow : 1.0.0 (Optionnel pour SQL)</p></li><li><p>Py4J       : 0.10.9 (Requis)</p></li><li><p>Java 8 ou recent avec JAVA_HOME correctement définit</p></li></ul></div></li></ul></div></div></section></section>
<section><section id="_interaction_avec_spark"><h2>Interaction avec Spark</h2><div class="slide-content"><div class="paragraph"><p>Installation en local, ou depuis la plateforme Databricks</p></div></div></section><section id="_️_interaction_avec_spark_standalone"><h2>▶️ Interaction avec Spark: Standalone</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://spark.apache.org/downloads.html">Téléchargez Apache Spark™</a></p></li></ul></div>
<div class="imageblock"><img src="media/j2/image_2023-01-27-01-25-54.png_.png" alt="image 2023 01 27 01 25 54.png "></div>
<div class="ulist"><ul><li><p><a href="https://spark.apache.org/docs/latest/api/python/getting_started/install.html">installation de Pyspark</a></p></li></ul></div>
<div class="imageblock"><img src="media/j2/image_2023-01-27-01-26-13.png_.png" alt="image 2023 01 27 01 26 13.png "></div></div></section><section id="_interaction_avec_spark_docker"><h2>Interaction avec Spark : Docker</h2><div class="slide-content"><div class="ulist"><ul><li><p>Télécharger Spark depuis la page de <a href="https://hub.docker.com/r/jupyter/pyspark-notebook">Docker PySpark Notebook</a></p><div class="imageblock"><img src="media/j2/image_2023-01-30-16-09-03.png_.png" alt="image 2023 01 30 16 09 03.png "></div>
<div class="listingblock"><div class="title">Télécharger l&#8217;image Docker de spark et lancer pyspark</div><div class="content"><pre class="highlightjs highlight"><code class="language-bash hljs" data-noescape="true" data-lang="bash">docker pull jupyter/pyspark-notebook

docker run -p 8888:8888  -e JUPYTER_ENABLE_LAB=yes
-v C:\datasicence\ressource:/home/jovyan/work --name pyspark jupyter/pyspark-notebook</code></pre></div></div></li></ul></div></div></section><section id="_interaction_avec_spark_docker_2" class="columns is-vcentered"><h2>Interaction avec Spark : Docker</h2><div class="slide-content"><div class="openblock column"><div class="content"><div class="imageblock"><img src="media/j2/image_2023-01-30-15-59-38.png_.png" alt="image 2023 01 30 15 59 38.png "></div><div class="title">Figure 5. docker pull jupyter/pyspark-notebook <sup class="footnote">[<span class="footnote" title="View footnote.">1</span>]</sup></div></div></div>
<div class="openblock column"><div class="content"><div class="imageblock"><img src="media/j2/image_2023-01-30-16-00-29.png_.png" alt="image 2023 01 30 16 00 29.png "></div></div></div></div><div class="footnotes"><div class="footnote">1. <a href="https://docs.docker.com/get-started/docker_cheatsheet.pdf" class="bare">docs.docker.com/get-started/docker_cheatsheet.pdf</a></div></div></section><section id="_interaction_avec_spark_jupyter_notebook_pyspark"><h2>Interaction avec Spark : Jupyter Notebook + PySpark</h2><div class="slide-content"><div class="imageblock"><img src="media/j2/image_2023-01-30-15-56-00.png_.png" alt="image 2023 01 30 15 56 00.png " height="600"></div>
<div class="listingblock"><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">from pyspark import SparkConf, SparkContext
conf = SparkConf().setMaster("local").setAppName("My App")
sc = SparkContext(conf = conf)</code></pre></div></div></div></section><section id="_interaction_avec_spark_standalone"><h2>Interaction avec Spark : Standalone</h2><div class="slide-content"><div class="admonitionblock warning"><table><tr><td class="icon"><i class="fa fa-warning" title="Warning"></i></td><td class="content"><div class="paragraph"><p>Il est risqué de définir plusieurs SparkContext, dans ce cas utilisez sc.stop() pour arrêter une session avant de démarrer une nouvelle.</p></div></td></tr></table></div>
<div class="admonitionblock important"><table><tr><td class="icon"><i class="fa fa-exclamation-circle" title="Important"></i></td><td class="content"><div class="title">Principales différences</div><div class="ulist"><ul><li><p><code>Spark Context</code> est à définir manuellement dans le code</p></li><li><p>Nécessité de lier les dépendances entre <code>Spark</code> et le programme (Java et <code>Scala</code>)</p></li><li><p><code>spark-submit</code> permet d&#8217;exécuter le script</p></li></ul></div></td></tr></table></div></div></section><section id="_️_interaction_avec_spark_databricks"><h2>▶️ Interaction avec Spark : Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p>Solution sur le cloud intégrant Spark: <a href="https://community.cloud.databricks.com/" class="bare">community.cloud.databricks.com/</a></p></li></ul></div>
<div class="imageblock"><img src="media/j2/image_2023-01-27-01-04-55.png_.png" alt="image 2023 01 27 01 04 55.png " height="700"></div></div></section><section id="_interaction_avec_spark_databricks"><h2>Interaction avec Spark : Databricks</h2><div class="slide-content"><div class="imageblock"><img src="media/j2/image_2023-01-27-01-06-02.png_.png" alt="image 2023 01 27 01 06 02.png " height="700"></div></div></section><section id="_interaction_avec_spark_databricks_2"><h2>Interaction avec Spark : Databricks</h2><div class="slide-content"><div class="imageblock"><img src="media/j2/image_2023-01-27-01-09-01.png_.png" alt="image 2023 01 27 01 09 01.png " height="700"></div></div></section></section>
<section><section id="_démo_de_spark"><h2>Démo de Spark</h2></section><section id="_️_manipulation_des_dataframes"><h2>▶️ Manipulation des DataFrames</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://mybinder.org/v2/gh/apache/spark/fbbcf9434a?filepath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_df.ipynb">Quickstart DataFrame Notebook</a></p></li><li><p><a href="https://mybinder.org/v2/gh/apache/spark/fbbcf9434a?filepath=python%2Fdocs%2Fsource%2Fgetting_started%2Fquickstart_ps.ipynb">Quickstart: Pandas API on Spark</a></p></li></ul></div></div></section><section id="_️_lecture_des_données_en_python"><h2>▶️ Lecture des données en Python</h2><div class="slide-content"><div class="ulist"><ul><li><p>Lien vers un fichier parquet d&#8217;Airbnb : <a href="https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet">sf-airbnb-clean.parquet</a></p></li></ul></div>
<div class="listingblock"><div class="title">Lecture des données en Python</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">filePath = "/databricks-datasets/learning-spark-v2/sf-airbnb/sf-airbnb-clean.parquet/"
airbnbDF = spark.read.parquet(filePath)

airbnbDF.select("neighbourhood_cleansed", "room_type", "bedrooms", "bathrooms", "number_of_reviews", "price").show(5)</code></pre></div></div></div></section><section id="_️_regression_logistic_avec_pyspark"><h2>▶️ Regression Logistic avec PySpark</h2><div class="slide-content"><div class="listingblock"><div class="title">Regression in Python</div><div class="content"><pre class="highlightjs highlight"><code class="language-python hljs" data-noescape="true" data-lang="python">from pyspark.ml.classification import LogisticRegression

training = spark.read.csv("s3://...")
test = spark.read.csv("s3://...")
# Load training data
lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)
# Fit the model
lrModel = lr.fit(training)
# Predict
lrModel.transform(test)</code></pre></div></div>
<div class="ulist"><ul><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/getting-started/dataframes-python" class="bare">learn.microsoft.com/fr-fr/azure/databricks/getting-started/dataframes-python</a></p></li><li><p><a href="https://learn.microsoft.com/fr-fr/azure/databricks/getting-started/free-training" class="bare">learn.microsoft.com/fr-fr/azure/databricks/getting-started/free-training</a></p></li></ul></div></div></section></section>
<section id="_quiz_2"><h2>Quiz</h2><div class="slide-content"><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:33.3333%"><col style="width:33.3333%"><col style="width:33.3334%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Quelle définition décrit le mieux Apache Spark ?</p><div class="olist arabic"><ol class="arabic"><li><p>Un système de gestion de base de données relationnelle hautement scalable.</p></li><li><p>Un serveur virtuel avec un runtime Python.</p></li><li><p>Une plateforme distribuée pour le traitement de données en parallèle à l’aide de plusieurs langages.</p></li></ol></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Vous devez utiliser Spark pour analyser les données dans un fichier parquet. Que devez-vous faire ?</p><div class="olist arabic"><ol class="arabic"><li><p>Charger le fichier parquet dans un dataframe.</p></li><li><p>Importer les données dans une table dans un pool SQL serverless.</p></li><li><p>Exporter les données de table au format CSV.</p></li></ol></div></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>Quelle commande magic utiliser pour écrire du code dans une cellule de notebook d&#8217;une requête SQL?</p><div class="olist arabic"><ol class="arabic"><li><p>%spark</p></li><li><p>%pyspark</p></li><li><p>%sql</p></li></ol></div></li></ul></div></div></td></tr></table></div></section>
<section><section id="_session_pratique_2"><h2>Session pratique</h2></section><section id="_programmation_dapache_spark_avec_databricks"><h2>Programmation d&#8217;Apache Spark avec Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p>Databricks Academy</p><div class="ulist"><ul><li><p>via <strong>Databricks Repo</strong>: <a href="https://github.com/databricks-academy/apache-spark-programming-with-databricks.git">Programmation d&#8217;Apache Spark avec Databricks GIT</a></p></li><li><p>ou bien directement via <strong>Databricks Archive</strong>: <a href="https://github.com/databricks-academy/apache-spark-programming-with-databricks/releases/download/v2.2.9/apache-spark-programming-with-databricks-v2.2.9-notebooks.dbc">Programmation d&#8217;Apache Spark avec Databricks DBC</a></p></li></ul></div></li></ul></div></div></section><section id="_apprendre_spark"><h2>Apprendre Spark</h2><div class="slide-content"><div class="ulist"><ul><li><p>Chapitre 01-10: <a href="https://github.com/databricks/LearningSparkV2/blob/master/notebooks/LearningSparkv2.dbc">Learning Spark DBC</a></p></li></ul></div></div></section><section id="_ml_avec_spark_dans_databricks"><h2>ML avec Spark dans Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://files.training.databricks.com/classes/ml/ml-on-spark.dbc">Machine Learning  on Spark DBC</a></p></li></ul></div>
<div class="imageblock"><img src="media/j2/image_2023-01-27-01-51-20.png_.png" alt="image 2023 01 27 01 51 20.png "></div></div></section><section id="_quickstart_au_machine_learning_training"><h2>Quickstart au Machine Learning training</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://learn.microsoft.com/en-us/azure/databricks/_extras/notebooks/source/mlflow/ml-quickstart-training.html">ml-quickstart-training</a></p></li></ul></div></div></section><section id="_hands_on_machine_learning_on_azure_databricks"><h2>Hands-on: Machine Learning on Azure Databricks</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://github.com/MicrosoftLearning/dp-090-databricks-ml/raw/master/02%20-%20Training%20and%20Evaluating%20Machine%20Learning%20Models.dbc">Training and Evaluating Machine Learning Models DBC</a></p></li><li><p><a href="https://github.com/MicrosoftLearning/dp-090-databricks-ml/raw/master/03%20-%20Managing%20Experiments%20and%20Models.dbc">Managing Experiments and Models DBC</a></p></li><li><p><a href="https://github.com/MicrosoftLearning/dp-090-databricks-ml/raw/master/04%20-%20Integrating%20Azure%20Databricks%20and%20Azure%20Machine%20Learning.dbc">Integrating Azure Databricks and Azure Machine Learning DBC</a></p></li></ul></div></div></section></section>
<section><section id="_ressources"><h2>Ressources</h2></section><section id="_️_livre_learning_spark"><h2>▶️ Livre: Learning Spark</h2><div class="slide-content"><div class="imageblock"><img src="media/j2/image_2023-01-26-21-46-22.png_.png" alt="image 2023 01 26 21 46 22.png " height="700"></div>
<div class="ulist"><ul><li><p><a href="https://www.amazon.fr/dp/1492050040?linkCode=gs2&amp;tag=oreilly2007-21">Learning Spark</a></p></li></ul></div></div></section><section id="_️_documentation_de_lapi_spark"><h2>▶️ Documentation de l’API Spark</h2><div class="slide-content"><div class="ulist"><ul><li><p>La page principale
<a href="https://spark.apache.org/docs/latest/">documentation</a> de Spark comprend des liens vers des documents d’API et des guides utiles pour chaque version de Spark.</p></li><li><p><a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html">API Scala</a> et
<a href="https://spark.apache.org/%20docs/latest/api/python/index.html">API Python</a> sont les plus couramment utilisés, et il est souvent utile de
référencer la documentation pour les deux langages.</p></li><li><p>Les documents Scala ont tendance à être plus complets et les documents Python ont tendance à avoir plus d’exemples de code.</p></li></ul></div></div></section><section id="_️_exercices_du_livre_learning_spark"><h2>▶️ Exercices du livre Learning Spark</h2><div class="slide-content"><div class="imageblock"><img src="media/j2/image_2023-01-26-21-47-46.png_.png" alt="image 2023 01 26 21 47 46.png " height="500"></div>
<div class="ulist"><ul><li><p><a href="https://github.com/databricks/LearningSparkV2/tree/master/notebooks" class="bare">github.com/databricks/LearningSparkV2/tree/master/notebooks</a></p></li></ul></div></div></section><section id="_️_spark_avec_des_exemples"><h2>▶️ Spark avec des exemples</h2><div class="slide-content"><div class="ulist"><ul><li><p><a href="https://sparkbyexamples.com/">sparkbyexamples</a></p></li></ul></div>
<div class="imageblock"><img src="media/j2/image_2023-01-27-01-28-49.png_.png" alt="image 2023 01 27 01 28 49.png "></div></div></section></section><footer>
  <p>spark avec databricks @ Amine HY, Copyright 2023</p>
  <p>spark avec databricks @ Amine HY, Copyright 2023</p>
</footer></div></div><script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2/js/reveal.js"></script><script>Array.prototype.slice.call(document.querySelectorAll('.slides section')).forEach(function(slide) {
  if (slide.getAttribute('data-background-color')) return;
  // user needs to explicitly say he wants CSS color to override otherwise we might break custom css or theme (#226)
  if (!(slide.classList.contains('canvas') || slide.classList.contains('background'))) return;
  var bgColor = getComputedStyle(slide).backgroundColor;
  if (bgColor !== 'rgba(0, 0, 0, 0)' && bgColor !== 'transparent') {
    slide.setAttribute('data-background-color', bgColor);
    slide.style.backgroundColor = 'transparent';
  }
});

// More info about config & dependencies:
// - https://github.com/hakimel/reveal.js#configuration
// - https://github.com/hakimel/reveal.js#dependencies
Reveal.initialize({
  // Display presentation control arrows
  controls: true,
  // Help the user learn the controls by providing hints, for example by
  // bouncing the down arrow when they first encounter a vertical slide
  controlsTutorial: true,
  // Determines where controls appear, "edges" or "bottom-right"
  controlsLayout: 'bottom-right',
  // Visibility rule for backwards navigation arrows; "faded", "hidden"
  // or "visible"
  controlsBackArrows: 'faded',
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: 'true',
  // Control which views the slide number displays on
  showSlideNumber: 'all',
  // Add the current slide number to the URL hash so that reloading the
  // page/copying the URL will return you to the same slide
  hash: false,
  // Push each slide change to the browser history. Implies `hash: true`
  history: true,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Disables the default reveal.js slide layout so that you can use custom CSS layout
  disableLayout: false,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // See https://github.com/hakimel/reveal.js/#navigation-mode
  navigationMode: 'default',
  // Randomizes the order of slides each time the presentation loads
  shuffle: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags whether to include the current fragment in the URL,
  // so that reloading brings you to the same fragment position
  fragmentInURL: false,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Flags if we should show a help overlay when the questionmark
  // key is pressed
  help: true,
  // Flags if speaker notes should be visible to all viewers
  showNotes: false,
  // Global override for autolaying embedded media (video/audio/iframe)
  // - null: Media will only autoplay if data-autoplay is present
  // - true: All media will autoplay, regardless of individual setting
  // - false: No media will autoplay, regardless of individual setting
  autoPlayMedia: null,
  // Global override for preloading lazy-loaded iframes
  // - null: Iframes with data-src AND data-preload will be loaded when within
  //   the viewDistance, iframes with only data-src will be loaded when visible
  // - true: All iframes with data-src will be loaded when within the viewDistance
  // - false: All iframes with data-src will be loaded only when visible
  preloadIframes: null,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Use this method for navigation when auto-sliding
  autoSlideMethod: Reveal.navigateNext,
  // Specify the average time in seconds that you think you will spend
  // presenting each slide. This is used to show a pacing timer in the
  // speaker view
  defaultTiming: 120,
  // Specify the total time in seconds that is available to
  // present.  If this is set to a nonzero value, the pacing
  // timer will work out the time available for each slide,
  // instead of using the defaultTiming value
  totalTime: 0,
  // Specify the minimum amount of time you want to allot to
  // each slide, if using the totalTime calculation method.  If
  // the automated time allocation causes slide pacing to fall
  // below this threshold, then you will see an alert in the
  // speaker notes window
  minimumTimePerSlide: 0,
  // Enable slide navigation via mouse wheel
  mouseWheel: true,
  // Hide cursor if inactive
  hideInactiveCursor: true,
  // Time before the cursor is hidden (in ms)
  hideCursorTime: 5000,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  // Add `data-preview-link` and `data-preview-link="false"` to customise each link
  // individually
  previewLinks: false,
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Number of slides away from the current that are visible on mobile
  // devices. It is advisable to set this to a lower number than
  // viewDistance in order to save resources.
  mobileViewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',
  // Number of pixels to move the parallax background per slide
  // - Calculated automatically unless specified
  // - Set to 0 to disable movement along an axis
  parallaxBackgroundHorizontal: null,
  parallaxBackgroundVertical: null,
  // The display mode that will be used to show slides
  display: 'block',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1920,
  height: 1080,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 2.0,

  // PDF Export Options
  // Put each fragment on a separate page
  pdfSeparateFragments: true,
  // For slides that do not fit on a page, max number of pages
  pdfMaxPagesPerSlide: 1,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2/plugin/zoom-js/zoom.js', async: true },
      { src: 'https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2/plugin/notes/notes.js', async: true }
  ],

  

});</script><script>var dom = {};
dom.slides = document.querySelector('.reveal .slides');

function getRemainingHeight(element, slideElement, height) {
  height = height || 0;
  if (element) {
    var newHeight, oldHeight = element.style.height;
    // Change the .stretch element height to 0 in order find the height of all
    // the other elements
    element.style.height = '0px';
    // In Overview mode, the parent (.slide) height is set of 700px.
    // Restore it temporarily to its natural height.
    slideElement.style.height = 'auto';
    newHeight = height - slideElement.offsetHeight;
    // Restore the old height, just in case
    element.style.height = oldHeight + 'px';
    // Clear the parent (.slide) height. .removeProperty works in IE9+
    slideElement.style.removeProperty('height');
    return newHeight;
  }
  return height;
}

function layoutSlideContents(width, height) {
  // Handle sizing of elements with the 'stretch' class
  toArray(dom.slides.querySelectorAll('section .stretch')).forEach(function (element) {
    // Determine how much vertical space we can use
    var limit = 5; // hard limit
    var parent = element.parentNode;
    while (parent.nodeName !== 'SECTION' && limit > 0) {
      parent = parent.parentNode;
      limit--;
    }
    if (limit === 0) {
      // unable to find parent, aborting!
      return;
    }
    var remainingHeight = getRemainingHeight(element, parent, height);
    // Consider the aspect ratio of media elements
    if (/(img|video)/gi.test(element.nodeName)) {
      var nw = element.naturalWidth || element.videoWidth, nh = element.naturalHeight || element.videoHeight;
      var es = Math.min(width / nw, remainingHeight / nh);
      element.style.width = (nw * es) + 'px';
      element.style.height = (nh * es) + 'px';
    } else {
      element.style.width = width + 'px';
      element.style.height = remainingHeight + 'px';
    }
  });
}

function toArray(o) {
  return Array.prototype.slice.call(o);
}

Reveal.addEventListener('slidechanged', function () {
  layoutSlideContents(1920, 1080)
});
Reveal.addEventListener('ready', function () {
  layoutSlideContents(1920, 1080)
});
Reveal.addEventListener('resize', function () {
  layoutSlideContents(1920, 1080)
});</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.9.2/lib/css/monokai.css"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>

<script>

/* highlightjs-line-numbers.js 2.6.0 | (C) 2018 Yauheni Pakala | MIT License | github.com/wcoder/highlightjs-line-numbers.js */
/* Edited by Hakim for reveal.js; removed async timeout */
!function(n,e){"use strict";function t(){var n=e.createElement("style");n.type="text/css",n.innerHTML=g(".{0}{border-collapse:collapse}.{0} td{padding:0}.{1}:before{content:attr({2})}",[v,L,b]),e.getElementsByTagName("head")[0].appendChild(n)}function r(t){"interactive"===e.readyState||"complete"===e.readyState?i(t):n.addEventListener("DOMContentLoaded",function(){i(t)})}function i(t){try{var r=e.querySelectorAll("code.hljs,code.nohighlight");for(var i in r)r.hasOwnProperty(i)&&l(r[i],t)}catch(o){n.console.error("LineNumbers error: ",o)}}function l(n,e){"object"==typeof n&&f(function(){n.innerHTML=s(n,e)})}function o(n,e){if("string"==typeof n){var t=document.createElement("code");return t.innerHTML=n,s(t,e)}}function s(n,e){e=e||{singleLine:!1};var t=e.singleLine?0:1;return c(n),a(n.innerHTML,t)}function a(n,e){var t=u(n);if(""===t[t.length-1].trim()&&t.pop(),t.length>e){for(var r="",i=0,l=t.length;i<l;i++)r+=g('<tr><td class="{0}"><div class="{1} {2}" {3}="{5}"></div></td><td class="{4}"><div class="{1}">{6}</div></td></tr>',[j,m,L,b,p,i+1,t[i].length>0?t[i]:" "]);return g('<table class="{0}">{1}</table>',[v,r])}return n}function c(n){var e=n.childNodes;for(var t in e)if(e.hasOwnProperty(t)){var r=e[t];h(r.textContent)>0&&(r.childNodes.length>0?c(r):d(r.parentNode))}}function d(n){var e=n.className;if(/hljs-/.test(e)){for(var t=u(n.innerHTML),r=0,i="";r<t.length;r++){var l=t[r].length>0?t[r]:" ";i+=g('<span class="{0}">{1}</span>\n',[e,l])}n.innerHTML=i.trim()}}function u(n){return 0===n.length?[]:n.split(y)}function h(n){return(n.trim().match(y)||[]).length}function f(e){e()}function g(n,e){return n.replace(/{(\d+)}/g,function(n,t){return e[t]?e[t]:n})}var v="hljs-ln",m="hljs-ln-line",p="hljs-ln-code",j="hljs-ln-numbers",L="hljs-ln-n",b="data-line-number",y=/\r\n|\r|\n/g;n.hljs?(n.hljs.initLineNumbersOnLoad=r,n.hljs.lineNumbersBlock=l,n.hljs.lineNumbersValue=o,t()):n.console.error("highlight.js not detected!")}(window,document);

/**
 * This reveal.js plugin is wrapper around the highlight.js
 * syntax highlighting library.
 */
(function( root, factory ) {
  if (typeof define === 'function' && define.amd) {
    root.RevealHighlight = factory();
  } else if( typeof exports === 'object' ) {
    module.exports = factory();
  } else {
    // Browser globals (root is window)
    root.RevealHighlight = factory();
  }
}( this, function() {

  // Function to perform a better "data-trim" on code snippets
  // Will slice an indentation amount on each line of the snippet (amount based on the line having the lowest indentation length)
  function betterTrim(snippetEl) {
    // Helper functions
    function trimLeft(val) {
      // Adapted from https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/Trim#Polyfill
      return val.replace(/^[\s\uFEFF\xA0]+/g, '');
    }
    function trimLineBreaks(input) {
      var lines = input.split('\n');

      // Trim line-breaks from the beginning
      for (var i = 0; i < lines.length; i++) {
        if (lines[i].trim() === '') {
          lines.splice(i--, 1);
        } else break;
      }

      // Trim line-breaks from the end
      for (var i = lines.length-1; i >= 0; i--) {
        if (lines[i].trim() === '') {
          lines.splice(i, 1);
        } else break;
      }

      return lines.join('\n');
    }

    // Main function for betterTrim()
    return (function(snippetEl) {
      var content = trimLineBreaks(snippetEl.innerHTML);
      var lines = content.split('\n');
      // Calculate the minimum amount to remove on each line start of the snippet (can be 0)
      var pad = lines.reduce(function(acc, line) {
        if (line.length > 0 && trimLeft(line).length > 0 && acc > line.length - trimLeft(line).length) {
          return line.length - trimLeft(line).length;
        }
        return acc;
      }, Number.POSITIVE_INFINITY);
      // Slice each line with this amount
      return lines.map(function(line, index) {
        return line.slice(pad);
      })
        .join('\n');
    })(snippetEl);
  }

  var RevealHighlight = {

    HIGHLIGHT_STEP_DELIMITER: '|',
    HIGHLIGHT_LINE_DELIMITER: ',',
    HIGHLIGHT_LINE_RANGE_DELIMITER: '-',

    init: function() {

      // Read the plugin config options and provide fallbacks
      var config = Reveal.getConfig().highlight || {};
      config.highlightOnLoad = typeof config.highlightOnLoad === 'boolean' ? config.highlightOnLoad : true;
      config.escapeHTML = typeof config.escapeHTML === 'boolean' ? config.escapeHTML : true;

      [].slice.call( document.querySelectorAll( '.reveal pre code' ) ).forEach( function( block ) {

        // Trim whitespace if the "data-trim" attribute is present
        if( block.hasAttribute( 'data-trim' ) && typeof block.innerHTML.trim === 'function' ) {
          block.innerHTML = betterTrim( block );
        }

        // Escape HTML tags unless the "data-noescape" attrbute is present
        if( config.escapeHTML && !block.hasAttribute( 'data-noescape' )) {
          block.innerHTML = block.innerHTML.replace( /</g,"&lt;").replace(/>/g, '&gt;' );
        }

        // Re-highlight when focus is lost (for contenteditable code)
        block.addEventListener( 'focusout', function( event ) {
          hljs.highlightBlock( event.currentTarget );
        }, false );

        if( config.highlightOnLoad ) {
          RevealHighlight.highlightBlock( block );
        }
      } );

    },

    /**
     * Highlights a code block. If the <code> node has the
     * 'data-line-numbers' attribute we also generate slide
     * numbers.
     *
     * If the block contains multiple line highlight steps,
     * we clone the block and create a fragment for each step.
     */
    highlightBlock: function( block ) {

      hljs.highlightBlock( block );

      // Don't generate line numbers for empty code blocks
      if( block.innerHTML.trim().length === 0 ) return;

      if( block.hasAttribute( 'data-line-numbers' ) ) {
        hljs.lineNumbersBlock( block, { singleLine: true } );

        // If there is at least one highlight step, generate
        // fragments
        var highlightSteps = RevealHighlight.deserializeHighlightSteps( block.getAttribute( 'data-line-numbers' ) );
        if( highlightSteps.length > 1 ) {

          // If the original code block has a fragment-index,
          // each clone should follow in an incremental sequence
          var fragmentIndex = parseInt( block.getAttribute( 'data-fragment-index' ), 10 );
          if( typeof fragmentIndex !== 'number' || isNaN( fragmentIndex ) ) {
            fragmentIndex = null;
          }

          // Generate fragments for all steps except the original block
          highlightSteps.slice(1).forEach( function( highlight ) {

            var fragmentBlock = block.cloneNode( true );
            fragmentBlock.setAttribute( 'data-line-numbers', RevealHighlight.serializeHighlightSteps( [ highlight ] ) );
            fragmentBlock.classList.add( 'fragment' );
            block.parentNode.appendChild( fragmentBlock );
            RevealHighlight.highlightLines( fragmentBlock );

            if( typeof fragmentIndex === 'number' ) {
              fragmentBlock.setAttribute( 'data-fragment-index', fragmentIndex );
              fragmentIndex += 1;
            }
            else {
              fragmentBlock.removeAttribute( 'data-fragment-index' );
            }

          } );

          block.removeAttribute( 'data-fragment-index' )
          block.setAttribute( 'data-line-numbers', RevealHighlight.serializeHighlightSteps( [ highlightSteps[0] ] ) );

        }

        RevealHighlight.highlightLines( block );

      }

    },

    /**
     * Visually emphasize specific lines within a code block.
     * This only works on blocks with line numbering turned on.
     *
     * @param {HTMLElement} block a <code> block
     * @param {String} [linesToHighlight] The lines that should be
     * highlighted in this format:
     * "1" 		= highlights line 1
     * "2,5"	= highlights lines 2 & 5
     * "2,5-7"	= highlights lines 2, 5, 6 & 7
     */
    highlightLines: function( block, linesToHighlight ) {

      var highlightSteps = RevealHighlight.deserializeHighlightSteps( linesToHighlight || block.getAttribute( 'data-line-numbers' ) );

      if( highlightSteps.length ) {

        highlightSteps[0].forEach( function( highlight ) {

          var elementsToHighlight = [];

          // Highlight a range
          if( typeof highlight.end === 'number' ) {
            elementsToHighlight = [].slice.call( block.querySelectorAll( 'table tr:nth-child(n+'+highlight.start+'):nth-child(-n+'+highlight.end+')' ) );
          }
          // Highlight a single line
          else if( typeof highlight.start === 'number' ) {
            elementsToHighlight = [].slice.call( block.querySelectorAll( 'table tr:nth-child('+highlight.start+')' ) );
          }

          if( elementsToHighlight.length ) {
            elementsToHighlight.forEach( function( lineElement ) {
              lineElement.classList.add( 'highlight-line' );
            } );

            block.classList.add( 'has-highlights' );
          }

        } );

      }

    },

    /**
     * Parses and formats a user-defined string of line
     * numbers to highlight.
     *
     * @example
     * RevealHighlight.deserializeHighlightSteps( '1,2|3,5-10' )
     * // [
     * //   [ { start: 1 }, { start: 2 } ],
     * //   [ { start: 3 }, { start: 5, end: 10 } ]
     * // ]
     */
    deserializeHighlightSteps: function( highlightSteps ) {

      // Remove whitespace
      highlightSteps = highlightSteps.replace( /\s/g, '' );

      // Divide up our line number groups
      highlightSteps = highlightSteps.split( RevealHighlight.HIGHLIGHT_STEP_DELIMITER );

      return highlightSteps.map( function( highlights ) {

        return highlights.split( RevealHighlight.HIGHLIGHT_LINE_DELIMITER ).map( function( highlight ) {

          // Parse valid line numbers
          if( /^[\d-]+$/.test( highlight ) ) {

            highlight = highlight.split( RevealHighlight.HIGHLIGHT_LINE_RANGE_DELIMITER );

            var lineStart = parseInt( highlight[0], 10 ),
              lineEnd = parseInt( highlight[1], 10 );

            if( isNaN( lineEnd ) ) {
              return {
                start: lineStart
              };
            }
            else {
              return {
                start: lineStart,
                end: lineEnd
              };
            }

          }
          // If no line numbers are provided, no code will be highlighted
          else {

            return {};

          }

        } );

      } );

    },

    /**
     * Serializes parsed line number data into a string so
     * that we can store it in the DOM.
     */
    serializeHighlightSteps: function( highlightSteps ) {

      return highlightSteps.map( function( highlights ) {

        return highlights.map( function( highlight ) {

          // Line range
          if( typeof highlight.end === 'number' ) {
            return highlight.start + RevealHighlight.HIGHLIGHT_LINE_RANGE_DELIMITER + highlight.end;
          }
          // Single line
          else if( typeof highlight.start === 'number' ) {
            return highlight.start;
          }
          // All lines
          else {
            return '';
          }

        } ).join( RevealHighlight.HIGHLIGHT_LINE_DELIMITER );

      } ).join( RevealHighlight.HIGHLIGHT_STEP_DELIMITER );

    }

  }

  Reveal.registerPlugin( 'highlight', RevealHighlight );

  return RevealHighlight;

}));
        
hljs.initHighlightingOnLoad();
</script></body></html>